diff --git a/lucene/src/test/org/apache/lucene/index/TestIndexWriter.java b/lucene/src/test/org/apache/lucene/index/TestIndexWriter.java
index 533f1d8..91e5491 100644
--- a/lucene/src/test/org/apache/lucene/index/TestIndexWriter.java
+++ b/lucene/src/test/org/apache/lucene/index/TestIndexWriter.java
@@ -21,53 +21,57 @@ import java.io.ByteArrayOutputStream;
 import java.io.File;
 import java.io.IOException;
 import java.io.PrintStream;
+import java.io.Reader;
 import java.io.StringReader;
-import java.util.List;
 import java.util.ArrayList;
 import java.util.Arrays;
-import java.util.Set;
-import java.util.HashSet;
+import java.util.Collections;
 import java.util.HashMap;
+import java.util.HashSet;
 import java.util.Iterator;
+import java.util.List;
 import java.util.Map;
 import java.util.Random;
-import java.util.Collections;
+import java.util.Set;
 import java.util.concurrent.atomic.AtomicBoolean;
 
-import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
 import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.Fieldable;
 import org.apache.lucene.document.Field.Index;
 import org.apache.lucene.document.Field.Store;
 import org.apache.lucene.document.Field.TermVector;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.Fieldable;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
 import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.PhraseQuery;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.ScoreDoc;
 import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.TopDocs;
 import org.apache.lucene.search.spans.SpanTermQuery;
 import org.apache.lucene.store.AlreadyClosedException;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IndexOutput;
 import org.apache.lucene.store.Lock;
 import org.apache.lucene.store.LockFactory;
-import org.apache.lucene.store.NoLockFactory;
 import org.apache.lucene.store.MockDirectoryWrapper;
+import org.apache.lucene.store.NoLockFactory;
 import org.apache.lucene.store.RAMDirectory;
 import org.apache.lucene.store.SingleInstanceLockFactory;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.ThreadInterruptedException;
 import org.apache.lucene.util.UnicodeUtil;
 import org.apache.lucene.util._TestUtil;
-import org.apache.lucene.util.ThreadInterruptedException;
-import org.apache.lucene.util.BytesRef;
 
 public class TestIndexWriter extends LuceneTestCase {
 
@@ -681,6 +685,122 @@ public class TestIndexWriter extends LuceneTestCase {
       dir.close();
     }
 
+    // Make sure it's OK to change RAM buffer size and
+    // maxBufferedDocs in a write session
+    public void testChangingRAMBuffer() throws IOException {
+      Directory dir = newDirectory();      
+      IndexWriter writer  = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));
+      writer.getConfig().setMaxBufferedDocs(10);
+      writer.getConfig().setRAMBufferSizeMB(IndexWriterConfig.DISABLE_AUTO_FLUSH);
+
+      int lastFlushCount = -1;
+      for(int j=1;j<52;j++) {
+        Document doc = new Document();
+        doc.add(new Field("field", "aaa" + j, Field.Store.YES, Field.Index.ANALYZED));
+        writer.addDocument(doc);
+        _TestUtil.syncConcurrentMerges(writer);
+        int flushCount = writer.getFlushCount();
+        if (j == 1)
+          lastFlushCount = flushCount;
+        else if (j < 10)
+          // No new files should be created
+          assertEquals(flushCount, lastFlushCount);
+        else if (10 == j) {
+          assertTrue(flushCount > lastFlushCount);
+          lastFlushCount = flushCount;
+          writer.getConfig().setRAMBufferSizeMB(0.000001);
+          writer.getConfig().setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH);
+        } else if (j < 20) {
+          assertTrue(flushCount > lastFlushCount);
+          lastFlushCount = flushCount;
+        } else if (20 == j) {
+          writer.getConfig().setRAMBufferSizeMB(16);
+          writer.getConfig().setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH);
+          lastFlushCount = flushCount;
+        } else if (j < 30) {
+          assertEquals(flushCount, lastFlushCount);
+        } else if (30 == j) {
+          writer.getConfig().setRAMBufferSizeMB(0.000001);
+          writer.getConfig().setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH);
+        } else if (j < 40) {
+          assertTrue(flushCount> lastFlushCount);
+          lastFlushCount = flushCount;
+        } else if (40 == j) {
+          writer.getConfig().setMaxBufferedDocs(10);
+          writer.getConfig().setRAMBufferSizeMB(IndexWriterConfig.DISABLE_AUTO_FLUSH);
+          lastFlushCount = flushCount;
+        } else if (j < 50) {
+          assertEquals(flushCount, lastFlushCount);
+          writer.getConfig().setMaxBufferedDocs(10);
+          writer.getConfig().setRAMBufferSizeMB(IndexWriterConfig.DISABLE_AUTO_FLUSH);
+        } else if (50 == j) {
+          assertTrue(flushCount > lastFlushCount);
+        }
+      }
+      writer.close();
+      dir.close();
+    }
+
+    public void testChangingRAMBuffer2() throws IOException {
+      Directory dir = newDirectory();      
+      IndexWriter writer  = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));
+      writer.getConfig().setMaxBufferedDocs(10);
+      writer.getConfig().setMaxBufferedDeleteTerms(10);
+      writer.getConfig().setRAMBufferSizeMB(IndexWriterConfig.DISABLE_AUTO_FLUSH);
+
+      for(int j=1;j<52;j++) {
+        Document doc = new Document();
+        doc.add(new Field("field", "aaa" + j, Field.Store.YES, Field.Index.ANALYZED));
+        writer.addDocument(doc);
+      }
+      
+      int lastFlushCount = -1;
+      for(int j=1;j<52;j++) {
+        writer.deleteDocuments(new Term("field", "aaa" + j));
+        _TestUtil.syncConcurrentMerges(writer);
+        int flushCount = writer.getFlushCount();
+        if (j == 1)
+          lastFlushCount = flushCount;
+        else if (j < 10) {
+          // No new files should be created
+          assertEquals(flushCount, lastFlushCount);
+        } else if (10 == j) {
+          assertTrue(flushCount > lastFlushCount);
+          lastFlushCount = flushCount;
+          writer.getConfig().setRAMBufferSizeMB(0.000001);
+          writer.getConfig().setMaxBufferedDeleteTerms(1);
+        } else if (j < 20) {
+          assertTrue(flushCount > lastFlushCount);
+          lastFlushCount = flushCount;
+        } else if (20 == j) {
+          writer.getConfig().setRAMBufferSizeMB(16);
+          writer.getConfig().setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH);
+          lastFlushCount = flushCount;
+        } else if (j < 30) {
+          assertEquals(flushCount, lastFlushCount);
+        } else if (30 == j) {
+          writer.getConfig().setRAMBufferSizeMB(0.000001);
+          writer.getConfig().setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH);
+          writer.getConfig().setMaxBufferedDeleteTerms(1);
+        } else if (j < 40) {
+          assertTrue(flushCount> lastFlushCount);
+          lastFlushCount = flushCount;
+        } else if (40 == j) {
+          writer.getConfig().setMaxBufferedDeleteTerms(10);
+          writer.getConfig().setRAMBufferSizeMB(IndexWriterConfig.DISABLE_AUTO_FLUSH);
+          lastFlushCount = flushCount;
+        } else if (j < 50) {
+          assertEquals(flushCount, lastFlushCount);
+          writer.getConfig().setMaxBufferedDeleteTerms(10);
+          writer.getConfig().setRAMBufferSizeMB(IndexWriterConfig.DISABLE_AUTO_FLUSH);
+        } else if (50 == j) {
+          assertTrue(flushCount > lastFlushCount);
+        }
+      }
+      writer.close();
+      dir.close();
+    }
+
     public void testDiverseDocs() throws IOException {
       MockDirectoryWrapper dir = newDirectory();
       IndexWriter writer  = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setRAMBufferSizeMB(0.5));
@@ -2549,7 +2669,7 @@ public class TestIndexWriter extends LuceneTestCase {
     final Random r = random;
 
     Directory dir = newDirectory();
-    FlushCountingIndexWriter w = new FlushCountingIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, false)).setRAMBufferSizeMB(0.5).setMaxBufferedDocs(-1).setMaxBufferedDeleteTerms(-1));
+    FlushCountingIndexWriter w = new FlushCountingIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, false)).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(-1).setMaxBufferedDeleteTerms(-1));
     w.setInfoStream(VERBOSE ? System.out : null);
     Document doc = new Document();
     doc.add(newField("field", "go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20", Field.Store.NO, Field.Index.ANALYZED));
@@ -2576,7 +2696,7 @@ public class TestIndexWriter extends LuceneTestCase {
           count++;
         }
       }
-      assertTrue("flush happened too quickly during " + (doIndexing ? "indexing" : "deleting") + " count=" + count, count > 1500);
+      assertTrue("flush happened too quickly during " + (doIndexing ? "indexing" : "deleting") + " count=" + count, count > 3000);
     }
     w.close();
     dir.close();
@@ -2912,4 +3032,123 @@ public class TestIndexWriter extends LuceneTestCase {
     w.close();
     dir.close();
   }
+
+  private static class StringSplitAnalyzer extends Analyzer {
+    @Override
+    public TokenStream tokenStream(String fieldName, Reader reader) {
+      return new StringSplitTokenizer(reader);
+    }
+  }
+
+  private static class StringSplitTokenizer extends Tokenizer {
+    private final String[] tokens;
+    private int upto = 0;
+    private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
+
+    public StringSplitTokenizer(Reader r) {
+      try {
+        final StringBuilder b = new StringBuilder();
+        final char[] buffer = new char[1024];
+        int n;
+        while((n = r.read(buffer)) != -1) {
+          b.append(buffer, 0, n);
+        }
+        tokens = b.toString().split(" ");
+      } catch (IOException ioe) {
+        throw new RuntimeException(ioe);
+      }
+    }
+
+    @Override
+    public final boolean incrementToken() throws IOException {
+      clearAttributes();      
+      if (upto < tokens.length) {
+        termAtt.setEmpty();
+        termAtt.append(tokens[upto]);
+        upto++;
+        return true;
+      } else {
+        return false;
+      }
+    }
+  }
+
+  /**
+   * Make sure we skip wicked long terms.
+   */
+  public void testWickedLongTerm() throws IOException {
+    Directory dir = newDirectory();
+    RandomIndexWriter w = new RandomIndexWriter(random, dir, new StringSplitAnalyzer());
+
+    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];
+    Arrays.fill(chars, 'x');
+    Document doc = new Document();
+    final String bigTerm = new String(chars);
+    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);
+
+    // This contents produces a too-long term:
+    String contents = "abc xyz x" + bigTerm + " another term";
+    doc.add(new Field("content", contents, Field.Store.NO, Field.Index.ANALYZED));
+    w.addDocument(doc);
+
+    // Make sure we can add another normal document
+    doc = new Document();
+    doc.add(new Field("content", "abc bbb ccc", Field.Store.NO, Field.Index.ANALYZED));
+    w.addDocument(doc);
+
+    IndexReader reader = w.getReader();
+    w.close();
+
+    // Make sure all terms < max size were indexed
+    assertEquals(2, reader.docFreq(new Term("content", "abc")));
+    assertEquals(1, reader.docFreq(new Term("content", "bbb")));
+    assertEquals(1, reader.docFreq(new Term("content", "term")));
+    assertEquals(1, reader.docFreq(new Term("content", "another")));
+
+    // Make sure position is still incremented when
+    // massive term is skipped:
+    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, "content", new BytesRef("another"));
+    assertEquals(0, tps.nextDoc());
+    assertEquals(1, tps.freq());
+    assertEquals(3, tps.nextPosition());
+
+    // Make sure the doc that has the massive term is in
+    // the index:
+    assertEquals("document with wicked long term should is not in the index!", 2, reader.numDocs());
+
+    reader.close();
+    dir.close();
+    dir = newDirectory();
+
+    // Make sure we can add a document with exactly the
+    // maximum length term, and search on that term:
+    doc = new Document();
+    Field contentField = new Field("content", "", Field.Store.NO, Field.Index.NOT_ANALYZED);
+    doc.add(contentField);
+
+    w = new RandomIndexWriter(random, dir);
+
+    contentField.setValue("other");
+    w.addDocument(doc);
+
+    contentField.setValue("term");
+    w.addDocument(doc);
+
+    contentField.setValue(bigTerm);
+    w.addDocument(doc);
+
+    contentField.setValue("zzz");
+    w.addDocument(doc);
+
+    reader = w.getReader();
+    w.close();
+    assertEquals(1, reader.docFreq(new Term("content", bigTerm)));
+
+    FieldCache.DocTermsIndex dti = FieldCache.DEFAULT.getTermsIndex(reader, "content", random.nextBoolean());
+    assertEquals(5, dti.numOrd());                // +1 for null ord
+    assertEquals(4, dti.size());
+    assertEquals(bigTermBytesRef, dti.lookup(3, new BytesRef()));
+    reader.close();
+    dir.close();
+  }
 }
