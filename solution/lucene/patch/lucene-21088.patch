diff --git a/lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter.java b/lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter.java
index 03c2b41..f5e978a 100644
--- a/lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter.java
+++ b/lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter.java
@@ -23,23 +23,24 @@ import java.util.Map;
 import org.apache.lucene.store.IndexOutput;
 import org.apache.lucene.util.ArrayUtil;
 import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.RamUsageEstimator;
 
 final class TermVectorsTermsWriter extends TermsHashConsumer {
 
   final DocumentsWriterPerThread docWriter;
-  TermVectorsWriter termVectorsWriter;
   int freeCount;
   IndexOutput tvx;
   IndexOutput tvd;
   IndexOutput tvf;
   int lastDocID;
-  
+
   final DocumentsWriterPerThread.DocState docState;
   final BytesRef flushTerm = new BytesRef();
-  
+
   // Used by perField when serializing the term vectors
   final ByteSliceReader vectorSliceReader = new ByteSliceReader();
+  boolean hasVectors;
 
   public TermVectorsTermsWriter(DocumentsWriterPerThread docWriter) {
     this.docWriter = docWriter;
@@ -48,35 +49,28 @@ final class TermVectorsTermsWriter extends TermsHashConsumer {
 
   @Override
   void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {
-
     if (tvx != null) {
+      // At least one doc in this run had term vectors enabled
+      fill(state.numDocs);
+      assert state.segmentName != null;
+      String idxName = IndexFileNames.segmentFileName(state.segmentName, "", IndexFileNames.VECTORS_INDEX_EXTENSION);
+      String fldName = IndexFileNames.segmentFileName(state.segmentName, "", IndexFileNames.VECTORS_FIELDS_EXTENSION);
+      String docName = IndexFileNames.segmentFileName(state.segmentName, "", IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);
 
-      if (state.numDocs > 0) {
-        // In case there are some final documents that we
-        // didn't see (because they hit a non-aborting exception):
-        fill(state.numDocs);
-      }
-
-      tvx.flush();
-      tvd.flush();
-      tvf.flush();
-      
       tvx.close();
       tvf.close();
       tvd.close();
       tvx = null;
-      String idxName = IndexFileNames.segmentFileName(state.segmentName, "", IndexFileNames.VECTORS_INDEX_EXTENSION);
       if (4+((long) state.numDocs)*16 != state.directory.fileLength(idxName))
         throw new RuntimeException("after flush: tvx size mismatch: " + state.numDocs + " docs vs " + state.directory.fileLength(idxName) + " length in bytes of " + idxName + " file exists?=" + state.directory.fileExists(idxName));
 
-      String fldName = IndexFileNames.segmentFileName(state.segmentName, "", IndexFileNames.VECTORS_FIELDS_EXTENSION);
-      String docName = IndexFileNames.segmentFileName(state.segmentName, "", IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);
       state.flushedFiles.add(idxName);
       state.flushedFiles.add(fldName);
       state.flushedFiles.add(docName);
 
       lastDocID = 0;
-
+      state.hasVectors = hasVectors;
+      hasVectors = false;
     }
 
     for (final TermsHashConsumerPerField field : fieldsToFlush.values() ) {
@@ -89,10 +83,9 @@ final class TermVectorsTermsWriter extends TermsHashConsumer {
   /** Fills in no-term-vectors for all docs we haven't seen
    *  since the last doc that had term vectors. */
   void fill(int docID) throws IOException {
-    final int end = docID;
-    if (lastDocID < end) {
+    if (lastDocID < docID) {
       final long tvfPosition = tvf.getFilePointer();
-      while(lastDocID < end) {
+      while(lastDocID < docID) {
         tvx.writeLong(tvd.getFilePointer());
         tvd.writeVInt(0);
         tvx.writeLong(tvfPosition);
@@ -101,25 +94,18 @@ final class TermVectorsTermsWriter extends TermsHashConsumer {
     }
   }
 
-  private final void initTermVectorsWriter() throws IOException {        
+  private final void initTermVectorsWriter() throws IOException {
     if (tvx == null) {
-      
-      final String segment = docWriter.getSegment();
-
-      if (segment == null)
-        return;
 
       // If we hit an exception while init'ing the term
       // vector output files, we must abort this segment
       // because those files will be in an unknown
       // state:
-      String idxName = IndexFileNames.segmentFileName(segment, "", IndexFileNames.VECTORS_INDEX_EXTENSION);
-      String docName = IndexFileNames.segmentFileName(segment, "", IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);
-      String fldName = IndexFileNames.segmentFileName(segment, "", IndexFileNames.VECTORS_FIELDS_EXTENSION);
-      tvx = docWriter.directory.createOutput(idxName);
-      tvd = docWriter.directory.createOutput(docName);
-      tvf = docWriter.directory.createOutput(fldName);
-      
+      hasVectors = true;
+      tvx = docWriter.directory.createOutput(IndexFileNames.segmentFileName(docWriter.getSegment(), "", IndexFileNames.VECTORS_INDEX_EXTENSION));
+      tvd = docWriter.directory.createOutput(IndexFileNames.segmentFileName(docWriter.getSegment(), "", IndexFileNames.VECTORS_DOCUMENTS_EXTENSION));
+      tvf = docWriter.directory.createOutput(IndexFileNames.segmentFileName(docWriter.getSegment(), "", IndexFileNames.VECTORS_FIELDS_EXTENSION));
+
       tvx.writeInt(TermVectorsReader.FORMAT_CURRENT);
       tvd.writeInt(TermVectorsReader.FORMAT_CURRENT);
       tvf.writeInt(TermVectorsReader.FORMAT_CURRENT);
@@ -136,7 +122,7 @@ final class TermVectorsTermsWriter extends TermsHashConsumer {
     initTermVectorsWriter();
 
     fill(docState.docID);
-    
+
     // Append term vectors to the real outputs:
     long pointer = tvd.getFilePointer();
     tvx.writeLong(pointer);
@@ -167,30 +153,26 @@ final class TermVectorsTermsWriter extends TermsHashConsumer {
 
   @Override
   public void abort() {
-
-    if (tvx != null) {
-      try {
-        tvx.close();
-      } catch (Throwable t) {
-      }
-      tvx = null;
+    hasVectors = false;
+    try {
+      IOUtils.closeSafely(tvx, tvd, tvf);
+    } catch (IOException ignored) {
     }
-    if (tvd != null) {
-      try {
-        tvd.close();
-      } catch (Throwable t) {
-      }
-      tvd = null;
+    try {
+      docWriter.directory.deleteFile(IndexFileNames.segmentFileName(docWriter.getSegment(), "", IndexFileNames.VECTORS_INDEX_EXTENSION));
+    } catch (IOException ignored) {
     }
-    if (tvf != null) {
-      try {
-        tvf.close();
-      } catch (Throwable t) {
-      }
-      tvf = null;
+    try {
+      docWriter.directory.deleteFile(IndexFileNames.segmentFileName(docWriter.getSegment(), "", IndexFileNames.VECTORS_DOCUMENTS_EXTENSION));
+    } catch (IOException ignored) {
+    }
+    try {
+      docWriter.directory.deleteFile(IndexFileNames.segmentFileName(docWriter.getSegment(), "", IndexFileNames.VECTORS_FIELDS_EXTENSION));
+    } catch (IOException ignored) {
     }
+    tvx = tvd = tvf = null;
     lastDocID = 0;
-    
+
     reset();
   }
 
@@ -210,7 +192,7 @@ final class TermVectorsTermsWriter extends TermsHashConsumer {
 
   void addFieldToFlush(TermVectorsTermsWriterPerField fieldToFlush) {
     if (numVectorFields == perFields.length) {
-      int newSize = ArrayUtil.oversize(numVectorFields + 1, RamUsageEstimator.NUM_BYTES_OBJ_REF);
+      int newSize = ArrayUtil.oversize(numVectorFields + 1, RamUsageEstimator.NUM_BYTES_OBJECT_REF);
       TermVectorsTermsWriterPerField[] newArray = new TermVectorsTermsWriterPerField[newSize];
       System.arraycopy(perFields, 0, newArray, 0, numVectorFields);
       perFields = newArray;
@@ -218,14 +200,14 @@ final class TermVectorsTermsWriter extends TermsHashConsumer {
 
     perFields[numVectorFields++] = fieldToFlush;
   }
-  
+
   @Override
   void startDocument() throws IOException {
     assert clearLastVectorFieldName();
     perFields = new TermVectorsTermsWriterPerField[1];
     reset();
   }
-  
+
   // Called only by assert
   final boolean clearLastVectorFieldName() {
     lastVectorFieldName = null;
