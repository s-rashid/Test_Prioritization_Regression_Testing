diff --git a/lucene/src/java/org/apache/lucene/index/DocumentsWriterPerThread.java b/lucene/src/java/org/apache/lucene/index/DocumentsWriterPerThread.java
index 36b5707..3dc25e5 100644
--- a/lucene/src/java/org/apache/lucene/index/DocumentsWriterPerThread.java
+++ b/lucene/src/java/org/apache/lucene/index/DocumentsWriterPerThread.java
@@ -7,9 +7,9 @@ package org.apache.lucene.index;
  * The ASF licenses this file to You under the Apache License, Version 2.0
  * (the "License"); you may not use this file except in compliance with
  * the License. You may obtain a copy of the License at
- * 
+ *
  * http://www.apache.org/licenses/LICENSE-2.0
- * 
+ *
  * Unless required by applicable law or agreed to in writing, software
  * distributed under the License is distributed on an "AS IS" BASIS,
  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -17,30 +17,35 @@ package org.apache.lucene.index;
  * limitations under the License.
  */
 
+import static org.apache.lucene.util.ByteBlockPool.BYTE_BLOCK_MASK;
+import static org.apache.lucene.util.ByteBlockPool.BYTE_BLOCK_SIZE;
+
 import java.io.IOException;
 import java.io.PrintStream;
+import java.text.NumberFormat;
+import java.util.ArrayList;
+import java.util.concurrent.atomic.AtomicLong;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.document.Document;
-import org.apache.lucene.index.codecs.Codec;
+import org.apache.lucene.search.Query;
 import org.apache.lucene.search.Similarity;
 import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.FilterDirectory;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.RamUsageEstimator;
+import org.apache.lucene.util.RecyclingByteBlockAllocator;
 
 public class DocumentsWriterPerThread {
-  
+
   /**
    * The IndexingChain must define the {@link #getChain(DocumentsWriter)} method
    * which returns the DocConsumer that the DocumentsWriter calls to process the
-   * documents. 
+   * documents.
    */
   abstract static class IndexingChain {
     abstract DocConsumer getChain(DocumentsWriterPerThread documentsWriterPerThread);
   }
 
-  
+
   static final IndexingChain defaultIndexingChain = new IndexingChain() {
 
     @Override
@@ -75,7 +80,10 @@ public class DocumentsWriterPerThread {
       return new DocFieldProcessor(documentsWriterPerThread, docInverter);
     }
   };
-  
+
+  // Deletes for our still-in-RAM (to be flushed next) segment
+  private SegmentDeletes pendingDeletes = new SegmentDeletes();
+
   static class DocState {
     final DocumentsWriterPerThread docWriter;
     Analyzer analyzer;
@@ -89,13 +97,13 @@ public class DocumentsWriterPerThread {
     DocState(DocumentsWriterPerThread docWriter) {
       this.docWriter = docWriter;
     }
-    
+
     // Only called by asserts
     public boolean testPoint(String name) {
       return docWriter.writer.testPoint(name);
     }
   }
-  
+
   /** Called if we hit an exception at a bad time (when
    *  updating the index files) and must discard all
    *  currently buffered docs.  This resets our state,
@@ -111,6 +119,7 @@ public class DocumentsWriterPerThread {
       } catch (Throwable t) {
       }
 
+      pendingDeletes.clear();
       // Reset all postings data
       doAfterFlush();
 
@@ -122,29 +131,26 @@ public class DocumentsWriterPerThread {
     }
   }
 
-  
-  final DocumentsWriterRAMAllocator ramAllocator = new DocumentsWriterRAMAllocator();
-
   final DocumentsWriter parent;
   final IndexWriter writer;
-  
+
   final Directory directory;
   final DocState docState;
   final DocConsumer consumer;
   private DocFieldProcessor docFieldProcessor;
-  
+
   String segment;                         // Current segment we are working on
   boolean aborting;               // True if an abort is pending
-  
+
   private final PrintStream infoStream;
   private int numDocsInRAM;
   private int flushedDocCount;
   SegmentWriteState flushState;
 
-  long[] sequenceIDs = new long[8];
-  
-  long numBytesUsed;
-  
+  final AtomicLong bytesUsed = new AtomicLong(0);
+
+  FieldInfos fieldInfos = new FieldInfos();
+
   public DocumentsWriterPerThread(Directory directory, DocumentsWriter parent, IndexingChain indexingChain) {
     parent.indexWriter.testPoint("DocumentsWriterPerThread.init start");
     this.directory = directory;
@@ -152,19 +158,19 @@ public class DocumentsWriterPerThread {
     this.writer = parent.indexWriter;
     this.infoStream = parent.indexWriter.getInfoStream();
     this.docState = new DocState(this);
-    this.docState.similarity = parent.config.getSimilarity();
-    this.docState.maxFieldLength = parent.config.getMaxFieldLength();
-    
+    this.docState.similarity = parent.indexWriter.getConfig().getSimilarity();
+    this.docState.maxFieldLength = IndexWriterConfig.UNLIMITED_FIELD_LENGTH;
+
     consumer = indexingChain.getChain(this);
     if (consumer instanceof DocFieldProcessor) {
       docFieldProcessor = (DocFieldProcessor) consumer;
     }
   }
-  
+
   void setAborting() {
     aborting = true;
   }
-  
+
   public void addDocument(Document doc, Analyzer analyzer) throws IOException {
     docState.doc = doc;
     docState.analyzer = analyzer;
@@ -174,17 +180,18 @@ public class DocumentsWriterPerThread {
       segment = writer.newSegmentName();
       assert numDocsInRAM == 0;
     }
-  
+
     boolean success = false;
     try {
       consumer.processDocument();
-      
+
       success = true;
     } finally {
       if (!success) {
         if (!aborting) {
           // mark document as deleted
-          commitDocument(-1);
+          deleteDocID(docState.docID);
+          commitDocument();
         }
       }
     }
@@ -192,90 +199,140 @@ public class DocumentsWriterPerThread {
     success = false;
     try {
       consumer.finishDocument();
-      
+
       success = true;
     } finally {
       if (!success) {
         setAborting();
       }
     }
+  }
 
+  void pushDeletes(SegmentInfo newSegment, SegmentInfos segmentInfos) {
+    // Lock order: DW -> BD
+    if (pendingDeletes.any()) {
+      if (newSegment != null) {
+        if (infoStream != null) {
+          message("flush: push buffered deletes to newSegment");
+        }
+        parent.bufferedDeletes.pushDeletes(pendingDeletes, newSegment);
+      } else if (segmentInfos.size() > 0) {
+        if (infoStream != null) {
+          message("flush: push buffered deletes to previously flushed segment " + segmentInfos.lastElement());
+        }
+        parent.bufferedDeletes.pushDeletes(pendingDeletes, segmentInfos.lastElement(), true);
+      } else {
+        if (infoStream != null) {
+          message("flush: drop buffered deletes: no segments");
+        }
+        // We can safely discard these deletes: since
+        // there are no segments, the deletions cannot
+        // affect anything.
+      }
+      pendingDeletes = new SegmentDeletes();
+    }
   }
 
-  public void commitDocument(long sequenceID) {
-    if (numDocsInRAM == sequenceIDs.length) {
-      sequenceIDs = ArrayUtil.grow(sequenceIDs);
+
+  // Buffer a specific docID for deletion.  Currently only
+  // used when we hit a exception when adding a document
+  synchronized void deleteDocID(int docIDUpto) {
+    pendingDeletes.addDocID(docIDUpto);
+    // NOTE: we do not trigger flush here.  This is
+    // potentially a RAM leak, if you have an app that tries
+    // to add docs but every single doc always hits a
+    // non-aborting exception.  Allowing a flush here gets
+    // very messy because we are only invoked when handling
+    // exceptions so to do this properly, while handling an
+    // exception we'd have to go off and flush new deletes
+    // which is risky (likely would hit some other
+    // confounding exception).
+  }
+
+  void deleteQueries(Query... queries) {
+    for (Query query : queries) {
+      pendingDeletes.addQuery(query, numDocsInRAM);
+    }
+  }
+
+  void deleteQuery(Query query) {
+    pendingDeletes.addQuery(query, numDocsInRAM);
+  }
+
+  void deleteTerms(Term... terms) {
+    for (Term term : terms) {
+      pendingDeletes.addTerm(term, numDocsInRAM);
     }
-    
-    sequenceIDs[numDocsInRAM] = sequenceID;
+  }
+
+  void deleteTerm(Term term) {
+    pendingDeletes.addTerm(term, numDocsInRAM);
+  }
+
+  public void commitDocument() {
     numDocsInRAM++;
   }
-  
+
   int getNumDocsInRAM() {
     return numDocsInRAM;
   }
-  
-  long getMinSequenceID() {
-    if (numDocsInRAM == 0) {
-      return -1;
-    }
-    return sequenceIDs[0];
-  }
-  
+
   /** Returns true if any of the fields in the current
   *  buffered docs have omitTermFreqAndPositions==false */
   boolean hasProx() {
     return (docFieldProcessor != null) ? docFieldProcessor.fieldInfos.hasProx()
                                       : true;
   }
-  
-  Codec getCodec() {
-    return flushState.codec;
+
+  SegmentCodecs getCodec() {
+    return flushState.segmentCodecs;
   }
-  
+
   /** Reset after a flush */
   private void doAfterFlush() throws IOException {
     segment = null;
     numDocsInRAM = 0;
   }
-    
+
   /** Flush all pending docs to a new segment */
   SegmentInfo flush() throws IOException {
     assert numDocsInRAM > 0;
 
-    flushState = new SegmentWriteState(infoStream, directory, segment, docFieldProcessor.fieldInfos,
+    flushState = new SegmentWriteState(infoStream, directory, segment, fieldInfos,
         numDocsInRAM, writer.getConfig().getTermIndexInterval(),
-        writer.codecs);
+        SegmentCodecs.build(fieldInfos, writer.codecs));
 
     if (infoStream != null) {
       message("flush postings as segment " + flushState.segmentName + " numDocs=" + numDocsInRAM);
     }
-    
+
     boolean success = false;
 
     try {
       consumer.flush(flushState);
 
+      boolean hasVectors = flushState.hasVectors;
+
       if (infoStream != null) {
         SegmentInfo si = new SegmentInfo(flushState.segmentName,
             flushState.numDocs,
             directory, false,
             hasProx(),
-            getCodec());
+            getCodec(),
+            hasVectors);
 
-        final long newSegmentSize = si.sizeInBytes();
-        String message = "  ramUsed=" + ramAllocator.nf.format(((double) numBytesUsed)/1024./1024.) + " MB" +
+        final long newSegmentSize = si.sizeInBytes(true);
+        String message = "  ramUsed=" + nf.format(((double) bytesUsed.get())/1024./1024.) + " MB" +
           " newFlushedSize=" + newSegmentSize +
-          " docs/MB=" + ramAllocator.nf.format(numDocsInRAM/(newSegmentSize/1024./1024.)) +
-          " new/old=" + ramAllocator.nf.format(100.0*newSegmentSize/numBytesUsed) + "%";
+          " docs/MB=" + nf.format(numDocsInRAM/(newSegmentSize/1024./1024.)) +
+          " new/old=" + nf.format(100.0*newSegmentSize/bytesUsed.get()) + "%";
         message(message);
       }
 
       flushedDocCount += flushState.numDocs;
 
-      long maxSequenceID = sequenceIDs[numDocsInRAM-1];
       doAfterFlush();
-      
+
       // Create new SegmentInfo, but do not add to our
       // segmentInfos until deletes are flushed
       // successfully.
@@ -283,12 +340,10 @@ public class DocumentsWriterPerThread {
                                    flushState.numDocs,
                                    directory, false,
                                    hasProx(),
-                                   getCodec());
+                                   getCodec(),
+                                   hasVectors);
+
 
-      
-      newSegment.setMinSequenceID(sequenceIDs[0]);
-      newSegment.setMaxSequenceID(maxSequenceID);
-      
       IndexWriter.setDiagnostics(newSegment, "flush");
       success = true;
 
@@ -304,12 +359,65 @@ public class DocumentsWriterPerThread {
   String getSegment() {
     return segment;
   }
-  
-  void bytesUsed(long numBytes) {
-    ramAllocator.bytesUsed(numBytes);
+
+  long bytesUsed() {
+    return bytesUsed.get();
+  }
+
+  FieldInfos getFieldInfos() {
+    return fieldInfos;
   }
-  
+
   void message(String message) {
     writer.message("DW: " + message);
   }
+
+  NumberFormat nf = NumberFormat.getInstance();
+
+  /* Initial chunks size of the shared byte[] blocks used to
+     store postings data */
+  final static int BYTE_BLOCK_NOT_MASK = ~BYTE_BLOCK_MASK;
+
+  /* if you increase this, you must fix field cache impl for
+   * getTerms/getTermsIndex requires <= 32768 */
+  final static int MAX_TERM_LENGTH_UTF8 = BYTE_BLOCK_SIZE-2;
+
+  /* Initial chunks size of the shared int[] blocks used to
+     store postings data */
+  final static int INT_BLOCK_SHIFT = 13;
+  final static int INT_BLOCK_SIZE = 1 << INT_BLOCK_SHIFT;
+  final static int INT_BLOCK_MASK = INT_BLOCK_SIZE - 1;
+
+  private ArrayList<int[]> freeIntBlocks = new ArrayList<int[]>();
+
+  /* Allocate another int[] from the shared pool */
+  synchronized int[] getIntBlock() {
+    final int size = freeIntBlocks.size();
+    final int[] b;
+    if (0 == size) {
+      b = new int[INT_BLOCK_SIZE];
+      bytesUsed.addAndGet(INT_BLOCK_SIZE*RamUsageEstimator.NUM_BYTES_INT);
+    } else
+      b = freeIntBlocks.remove(size-1);
+    return b;
+  }
+
+  /* Return int[]s to the pool */
+  synchronized void recycleIntBlocks(int[][] blocks, int start, int end) {
+    for(int i=start;i<end;i++) {
+      freeIntBlocks.add(blocks[i]);
+      blocks[i] = null;
+    }
+  }
+
+  final RecyclingByteBlockAllocator byteBlockAllocator = new RecyclingByteBlockAllocator(BYTE_BLOCK_SIZE, Integer.MAX_VALUE, bytesUsed);
+
+  final static int PER_DOC_BLOCK_SIZE = 1024;
+
+  final RecyclingByteBlockAllocator perDocAllocator = new RecyclingByteBlockAllocator(PER_DOC_BLOCK_SIZE, Integer.MAX_VALUE, bytesUsed);
+
+  String toMB(long v) {
+    return nf.format(v/1024./1024.);
+  }
+
 }
