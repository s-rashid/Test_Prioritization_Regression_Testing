diff --git a/lucene/src/java/org/apache/lucene/index/DocFieldProcessorPerThread.java b/lucene/src/java/org/apache/lucene/index/DocFieldProcessorPerThread.java
index 56e7dea..0f2fed9 100644
--- a/lucene/src/java/org/apache/lucene/index/DocFieldProcessorPerThread.java
+++ b/lucene/src/java/org/apache/lucene/index/DocFieldProcessorPerThread.java
@@ -20,14 +20,12 @@ package org.apache.lucene.index;
 import java.util.Collection;
 import java.util.HashSet;
 import java.util.List;
-import java.util.Set;
-import java.util.Map.Entry;
 import java.io.IOException;
 
-import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Fieldable;
 import org.apache.lucene.index.values.ValuesAttribute;
+import org.apache.lucene.index.values.codec.DocValuesConsumer;
 import org.apache.lucene.util.ArrayUtil;
 import org.apache.lucene.util.AttributeSource;
 import org.apache.lucene.util.RamUsageEstimator;
@@ -255,17 +253,16 @@ final class DocFieldProcessorPerThread extends DocConsumerPerThread {
       final DocFieldProcessorPerField perField = fields[i];
       final Fieldable fieldable = perField.fields[0];
       perField.consumer.processFields(perField.fields, perField.fieldCount);
+     
       if(!fieldable.hasFieldAttribute())
         continue;
       final AttributeSource attrSource = fieldable.getFieldAttributes();
       if(!attrSource.hasAttribute(ValuesAttribute.class))
         continue;
       final ValuesAttribute attribute = attrSource.getAttribute(ValuesAttribute.class);
-      final DocFieldProcessor.IndexValuesProcessor processor = docFieldProcessor
-          .getProcessor(docState.docWriter.directory,
+      final DocValuesConsumer consumer = docFieldProcessor.docValuesConsumer(docState.docWriter.directory,
               docState.docWriter.segment, fieldable.name(), attribute, perField.fieldInfo);
-      if (processor != null)
-        processor.add(docState.docID, fieldable.name(), attribute);
+      consumer.add(docState.docID, attribute);
     }
     if (docState.maxTermPrefix != null && docState.infoStream != null) {
       docState.infoStream.println("WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length " + DocumentsWriter.MAX_TERM_LENGTH_UTF8 + "), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '" + docState.maxTermPrefix + "...'");
