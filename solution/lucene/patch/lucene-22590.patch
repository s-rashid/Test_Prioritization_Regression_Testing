diff --git a/lucene/src/java/org/apache/lucene/index/SegmentReader.java b/lucene/src/java/org/apache/lucene/index/SegmentReader.java
index 0aa9448..b04886d 100644
--- a/lucene/src/java/org/apache/lucene/index/SegmentReader.java
+++ b/lucene/src/java/org/apache/lucene/index/SegmentReader.java
@@ -29,17 +29,16 @@ import java.util.concurrent.atomic.AtomicInteger;
 
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.FieldSelector;
-import org.apache.lucene.index.codecs.FieldsProducer;
 import org.apache.lucene.index.codecs.PerDocValues;
+import org.apache.lucene.index.values.DocValues;
 import org.apache.lucene.store.BufferedIndexInput;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
 import org.apache.lucene.util.BitVector;
 import org.apache.lucene.util.Bits;
-import org.apache.lucene.index.values.DocValues;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.CloseableThreadLocal;
+import org.apache.lucene.util.StringHelper;
 
 /**
  * @lucene.experimental
@@ -69,183 +68,10 @@ public class SegmentReader extends IndexReader implements Cloneable {
   private int rollbackPendingDeleteCount;
 
   // optionally used for the .nrm file shared by multiple norms
-  private IndexInput singleNormStream;
-  private AtomicInteger singleNormRef;
-
-  CoreReaders core;
-
-  // Holds core readers that are shared (unchanged) when
-  // SegmentReader is cloned or reopened
-  static final class CoreReaders {
-
-    // Counts how many other reader share the core objects
-    // (freqStream, proxStream, tis, etc.) of this reader;
-    // when coreRef drops to 0, these core objects may be
-    // closed.  A given instance of SegmentReader may be
-    // closed, even those it shares core objects with other
-    // SegmentReaders:
-    private final AtomicInteger ref = new AtomicInteger(1);
-
-    final String segment;
-    final FieldInfos fieldInfos;
-
-    final FieldsProducer fields;
-    final PerDocValues perDocProducer;
-    
-    final Directory dir;
-    final Directory cfsDir;
-    final int readBufferSize;
-    final int termsIndexDivisor;
-
-    private final SegmentReader origInstance;
-
-    FieldsReader fieldsReaderOrig;
-    TermVectorsReader termVectorsReaderOrig;
-    CompoundFileReader cfsReader;
-    CompoundFileReader storeCFSReader;
-
-    CoreReaders(SegmentReader origInstance, Directory dir, SegmentInfo si, int readBufferSize, int termsIndexDivisor) throws IOException {
-
-      if (termsIndexDivisor == 0) {
-        throw new IllegalArgumentException("indexDivisor must be < 0 (don't load terms index) or greater than 0 (got 0)");
-      }
-
-      segment = si.name;
-      final SegmentCodecs segmentCodecs = si.getSegmentCodecs();
-      this.readBufferSize = readBufferSize;
-      this.dir = dir;
-
-      boolean success = false;
-
-      try {
-        Directory dir0 = dir;
-        if (si.getUseCompoundFile()) {
-          cfsReader = new CompoundFileReader(dir, IndexFileNames.segmentFileName(segment, "", IndexFileNames.COMPOUND_FILE_EXTENSION), readBufferSize);
-          dir0 = cfsReader;
-        }
-        cfsDir = dir0;
-        si.loadFieldInfos(cfsDir, false); // prevent opening the CFS to load fieldInfos
-        fieldInfos = si.getFieldInfos();
-        
-        this.termsIndexDivisor = termsIndexDivisor;
-        
-        // Ask codec for its Fields
-        final SegmentReadState segmentReadState = new SegmentReadState(cfsDir, si, fieldInfos, readBufferSize, termsIndexDivisor);
-        fields = segmentCodecs.codec().fieldsProducer(segmentReadState);
-        assert fields != null;
-        perDocProducer = segmentCodecs.codec().docsProducer(segmentReadState);
-        success = true;
-      } finally {
-        if (!success) {
-          decRef();
-        }
-      }
+  IndexInput singleNormStream;
+  AtomicInteger singleNormRef;
 
-      // Must assign this at the end -- if we hit an
-      // exception above core, we don't want to attempt to
-      // purge the FieldCache (will hit NPE because core is
-      // not assigned yet).
-      this.origInstance = origInstance;
-    }
-    
-    
-
-    synchronized TermVectorsReader getTermVectorsReaderOrig() {
-      return termVectorsReaderOrig;
-    }
-
-    synchronized FieldsReader getFieldsReaderOrig() {
-      return fieldsReaderOrig;
-    }
-
-    synchronized void incRef() {
-      ref.incrementAndGet();
-    }
-
-    synchronized Directory getCFSReader() {
-      return cfsReader;
-    }
-
-    synchronized void decRef() throws IOException {
-      if (ref.decrementAndGet() == 0) {
-        if (fields != null) {
-          fields.close();
-        }
-        
-        if (perDocProducer != null) {
-          perDocProducer.close();
-        }
-
-        if (termVectorsReaderOrig != null) {
-          termVectorsReaderOrig.close();
-        }
-  
-        if (fieldsReaderOrig != null) {
-          fieldsReaderOrig.close();
-        }
-  
-        if (cfsReader != null) {
-          cfsReader.close();
-        }
-  
-        if (storeCFSReader != null) {
-          storeCFSReader.close();
-        }
-
-        // Now, notify any ReaderFinished listeners:
-        if (origInstance != null) {
-          origInstance.notifyReaderFinishedListeners();
-        }
-      }
-    }
-
-    synchronized void openDocStores(SegmentInfo si) throws IOException {
-
-      assert si.name.equals(segment);
-
-      if (fieldsReaderOrig == null) {
-        final Directory storeDir;
-        if (si.getDocStoreOffset() != -1) {
-          if (si.getDocStoreIsCompoundFile()) {
-            assert storeCFSReader == null;
-            storeCFSReader = new CompoundFileReader(dir,
-                IndexFileNames.segmentFileName(si.getDocStoreSegment(), "", IndexFileNames.COMPOUND_FILE_STORE_EXTENSION),
-                                                    readBufferSize);
-            storeDir = storeCFSReader;
-            assert storeDir != null;
-          } else {
-            storeDir = dir;
-            assert storeDir != null;
-          }
-        } else if (si.getUseCompoundFile()) {
-          // In some cases, we were originally opened when CFS
-          // was not used, but then we are asked to open doc
-          // stores after the segment has switched to CFS
-          if (cfsReader == null) {
-            cfsReader = new CompoundFileReader(dir, IndexFileNames.segmentFileName(segment, "", IndexFileNames.COMPOUND_FILE_EXTENSION), readBufferSize);
-          }
-          storeDir = cfsReader;
-          assert storeDir != null;
-        } else {
-          storeDir = dir;
-          assert storeDir != null;
-        }
-
-        final String storesSegment = si.getDocStoreSegment();
-        fieldsReaderOrig = new FieldsReader(storeDir, storesSegment, fieldInfos, readBufferSize,
-                                            si.getDocStoreOffset(), si.docCount);
-
-        // Verify two sources of "maxDoc" agree:
-        if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {
-          throw new CorruptIndexException("doc counts differ for segment " + segment + ": fieldsReader shows " + fieldsReaderOrig.size() + " but segmentInfo shows " + si.docCount);
-        }
-
-        if (si.getHasVectors()) { // open term vector files only as needed
-          termVectorsReaderOrig = new TermVectorsReader(storeDir, storesSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);
-        }
-      }
-    }
-  }
+  SegmentCoreReaders core;
 
   /**
    * Sets the initial value 
@@ -256,219 +82,8 @@ public class SegmentReader extends IndexReader implements Cloneable {
       return (FieldsReader) core.getFieldsReaderOrig().clone();
     }
   }
-  
-  /**
-   * Byte[] referencing is used because a new norm object needs 
-   * to be created for each clone, and the byte array is all 
-   * that is needed for sharing between cloned readers.  The 
-   * current norm referencing is for sharing between readers 
-   * whereas the byte[] referencing is for copy on write which 
-   * is independent of reader references (i.e. incRef, decRef).
-   */
-
-  final class Norm implements Cloneable {
-    private int refCount = 1;
-
-    // If this instance is a clone, the originalNorm
-    // references the Norm that has a real open IndexInput:
-    private Norm origNorm;
-
-    private IndexInput in;
-    private long normSeek;
 
-    // null until bytes is set
-    private AtomicInteger bytesRef;
-    private byte[] bytes;
-    private boolean dirty;
-    private int number;
-    private boolean rollbackDirty;
-    
-    public Norm(IndexInput in, int number, long normSeek) {
-      this.in = in;
-      this.number = number;
-      this.normSeek = normSeek;
-    }
-
-    public synchronized void incRef() {
-      assert refCount > 0 && (origNorm == null || origNorm.refCount > 0);
-      refCount++;
-    }
-
-    private void closeInput() throws IOException {
-      if (in != null) {
-        if (in != singleNormStream) {
-          // It's private to us -- just close it
-          in.close();
-        } else {
-          // We are sharing this with others -- decRef and
-          // maybe close the shared norm stream
-          if (singleNormRef.decrementAndGet() == 0) {
-            singleNormStream.close();
-            singleNormStream = null;
-          }
-        }
-
-        in = null;
-      }
-    }
-
-    public synchronized void decRef() throws IOException {
-      assert refCount > 0 && (origNorm == null || origNorm.refCount > 0);
-
-      if (--refCount == 0) {
-        if (origNorm != null) {
-          origNorm.decRef();
-          origNorm = null;
-        } else {
-          closeInput();
-        }
-
-        if (bytes != null) {
-          assert bytesRef != null;
-          bytesRef.decrementAndGet();
-          bytes = null;
-          bytesRef = null;
-        } else {
-          assert bytesRef == null;
-        }
-      }
-    }
-
-    // Load & cache full bytes array.  Returns bytes.
-    public synchronized byte[] bytes() throws IOException {
-      assert refCount > 0 && (origNorm == null || origNorm.refCount > 0);
-      if (bytes == null) {                     // value not yet read
-        assert bytesRef == null;
-        if (origNorm != null) {
-          // Ask origNorm to load so that for a series of
-          // reopened readers we share a single read-only
-          // byte[]
-          bytes = origNorm.bytes();
-          bytesRef = origNorm.bytesRef;
-          bytesRef.incrementAndGet();
-
-          // Once we've loaded the bytes we no longer need
-          // origNorm:
-          origNorm.decRef();
-          origNorm = null;
-
-        } else {
-          // We are the origNorm, so load the bytes for real
-          // ourself:
-          final int count = maxDoc();
-          bytes = new byte[count];
-
-          // Since we are orig, in must not be null
-          assert in != null;
-
-          // Read from disk.
-          synchronized(in) {
-            in.seek(normSeek);
-            in.readBytes(bytes, 0, count, false);
-          }
-
-          bytesRef = new AtomicInteger(1);
-          closeInput();
-        }
-      }
-
-      return bytes;
-    }
-
-    // Only for testing
-    AtomicInteger bytesRef() {
-      return bytesRef;
-    }
-
-    // Called if we intend to change a norm value.  We make a
-    // private copy of bytes if it's shared with others:
-    public synchronized byte[] copyOnWrite() throws IOException {
-      assert refCount > 0 && (origNorm == null || origNorm.refCount > 0);
-      bytes();
-      assert bytes != null;
-      assert bytesRef != null;
-      if (bytesRef.get() > 1) {
-        // I cannot be the origNorm for another norm
-        // instance if I'm being changed.  Ie, only the
-        // "head Norm" can be changed:
-        assert refCount == 1;
-        final AtomicInteger oldRef = bytesRef;
-        bytes = cloneNormBytes(bytes);
-        bytesRef = new AtomicInteger(1);
-        oldRef.decrementAndGet();
-      }
-      dirty = true;
-      return bytes;
-    }
-    
-    // Returns a copy of this Norm instance that shares
-    // IndexInput & bytes with the original one
-    @Override
-    public synchronized Object clone() {
-      assert refCount > 0 && (origNorm == null || origNorm.refCount > 0);
-        
-      Norm clone;
-      try {
-        clone = (Norm) super.clone();
-      } catch (CloneNotSupportedException cnse) {
-        // Cannot happen
-        throw new RuntimeException("unexpected CloneNotSupportedException", cnse);
-      }
-      clone.refCount = 1;
-
-      if (bytes != null) {
-        assert bytesRef != null;
-        assert origNorm == null;
-
-        // Clone holds a reference to my bytes:
-        clone.bytesRef.incrementAndGet();
-      } else {
-        assert bytesRef == null;
-        if (origNorm == null) {
-          // I become the origNorm for the clone:
-          clone.origNorm = this;
-        }
-        clone.origNorm.incRef();
-      }
-
-      // Only the origNorm will actually readBytes from in:
-      clone.in = null;
-
-      return clone;
-    }
-
-    // Flush all pending changes to the next generation
-    // separate norms file.
-    public void reWrite(SegmentInfo si) throws IOException {
-      assert refCount > 0 && (origNorm == null || origNorm.refCount > 0): "refCount=" + refCount + " origNorm=" + origNorm;
-
-      // NOTE: norms are re-written in regular directory, not cfs
-      si.advanceNormGen(this.number);
-      final String normFileName = si.getNormFileName(this.number);
-      IndexOutput out = directory().createOutput(normFileName);
-      boolean success = false;
-      try {
-        try {
-          out.writeBytes(bytes, maxDoc());
-        } finally {
-          out.close();
-        }
-        success = true;
-      } finally {
-        if (!success) {
-          try {
-            directory().deleteFile(normFileName);
-          } catch (Throwable t) {
-            // suppress this so we keep throwing the
-            // original exception
-          }
-        }
-      }
-      this.dirty = false;
-    }
-  }
-
-  Map<String,Norm> norms = new HashMap<String,Norm>();
+  Map<String,SegmentNorms> norms = new HashMap<String,SegmentNorms>();
   
   /**
    * @throws CorruptIndexException if the index is corrupt
@@ -498,7 +113,7 @@ public class SegmentReader extends IndexReader implements Cloneable {
     boolean success = false;
 
     try {
-      instance.core = new CoreReaders(instance, dir, si, readBufferSize, termInfosIndexDivisor);
+      instance.core = new SegmentCoreReaders(instance, dir, si, readBufferSize, termInfosIndexDivisor);
       if (doOpenStores) {
         instance.core.openDocStores(si);
       }
@@ -666,16 +281,16 @@ public class SegmentReader extends IndexReader implements Cloneable {
         }
       }
 
-      clone.norms = new HashMap<String,Norm>();
+      clone.norms = new HashMap<String,SegmentNorms>();
 
       // Clone norms
       for (FieldInfo fi : core.fieldInfos) {
         // Clone unchanged norms to the cloned reader
         if (doClone || !fieldNormsChanged.contains(fi.number)) {
           final String curField = fi.name;
-          Norm norm = this.norms.get(curField);
+          SegmentNorms norm = this.norms.get(curField);
           if (norm != null)
-            clone.norms.put(curField, (Norm) norm.clone());
+            clone.norms.put(curField, (SegmentNorms) norm.clone());
         }
       }
 
@@ -745,7 +360,7 @@ public class SegmentReader extends IndexReader implements Cloneable {
 
     if (normsDirty) {               // re-write norms
       si.initNormGen();
-      for (final Norm norm : norms.values()) {
+      for (final SegmentNorms norm : norms.values()) {
         if (norm.dirty) {
           norm.reWrite(si);
         }
@@ -771,7 +386,7 @@ public class SegmentReader extends IndexReader implements Cloneable {
       deletedDocs = null;
     }
 
-    for (final Norm norm : norms.values()) {
+    for (final SegmentNorms norm : norms.values()) {
       norm.decRef();
     }
     if (core != null) {
@@ -928,9 +543,6 @@ public class SegmentReader extends IndexReader implements Cloneable {
                 fieldOption == IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET) {
         fieldSet.add(fi.name);
       }
-      else if (fi.docValues != null && fieldOption == IndexReader.FieldOption.DOC_VALUES) {
-        fieldSet.add(fi.name);
-      }
     }
     return fieldSet;
   }
@@ -944,7 +556,7 @@ public class SegmentReader extends IndexReader implements Cloneable {
   @Override
   public byte[] norms(String field) throws IOException {
     ensureOpen();
-    final Norm norm = norms.get(field);
+    final SegmentNorms norm = norms.get(field);
     if (norm == null) {
       // not indexed, or norms not stored
       return null;  
@@ -955,7 +567,7 @@ public class SegmentReader extends IndexReader implements Cloneable {
   @Override
   protected void doSetNorm(int doc, String field, byte value)
           throws IOException {
-    Norm norm = norms.get(field);
+    SegmentNorms norm = norms.get(field);
     if (norm == null)                             // not an indexed field
       return;
 
@@ -997,11 +609,23 @@ public class SegmentReader extends IndexReader implements Cloneable {
           // If this were to change in the future, a clone could be done here.
           normInput = singleNormStream;
         } else {
-          normSeek = 0;
           normInput = d.openInput(fileName);
+          // if the segment was created in 3.2 or after, we wrote the header for sure,
+          // and don't need to do the sketchy file size check. otherwise, we check 
+          // if the size is exactly equal to maxDoc to detect a headerless file.
+          // NOTE: remove this check in Lucene 5.0!
+          String version = si.getVersion();
+          final boolean isUnversioned = 
+            (version == null || StringHelper.getVersionComparator().compare(version, "3.2") < 0)
+            && normInput.length() == maxDoc();
+          if (isUnversioned) {
+            normSeek = 0;
+          } else {
+            normSeek = SegmentMerger.NORMS_HEADER.length;
+          }
         }
 
-        norms.put(fi.name, new Norm(normInput, fi.number, normSeek));
+        norms.put(fi.name, new SegmentNorms(normInput, fi.number, normSeek, this));
         nextNormSeek += maxDoc; // increment also if some norms are separate
       }
     }
@@ -1021,7 +645,7 @@ public class SegmentReader extends IndexReader implements Cloneable {
     if (singleNormStream != null) {
       return false;
     }
-    for (final Norm norm : norms.values()) {
+    for (final SegmentNorms norm : norms.values()) {
       if (norm.refCount > 0) {
         return false;
       }
@@ -1168,7 +792,7 @@ public class SegmentReader extends IndexReader implements Cloneable {
     rollbackDeletedDocsDirty = deletedDocsDirty;
     rollbackNormsDirty = normsDirty;
     rollbackPendingDeleteCount = pendingDeleteCount;
-    for (Norm norm : norms.values()) {
+    for (SegmentNorms norm : norms.values()) {
       norm.rollbackDirty = norm.dirty;
     }
   }
@@ -1179,7 +803,7 @@ public class SegmentReader extends IndexReader implements Cloneable {
     deletedDocsDirty = rollbackDeletedDocsDirty;
     normsDirty = rollbackNormsDirty;
     pendingDeleteCount = rollbackPendingDeleteCount;
-    for (Norm norm : norms.values()) {
+    for (SegmentNorms norm : norms.values()) {
       norm.dirty = norm.rollbackDirty;
     }
   }
@@ -1205,8 +829,8 @@ public class SegmentReader extends IndexReader implements Cloneable {
   public int getTermInfosIndexDivisor() {
     return core.termsIndexDivisor;
   }
-  
-   @Override
+
+  @Override
   protected void readerFinished() {
     // Do nothing here -- we have more careful control on
     // when to notify that a SegmentReader has finished,
@@ -1215,6 +839,7 @@ public class SegmentReader extends IndexReader implements Cloneable {
     // longer used (all SegmentReaders sharing it have been
     // closed).
   }
+
   
   @Override
   public DocValues docValues(String field) throws IOException {
