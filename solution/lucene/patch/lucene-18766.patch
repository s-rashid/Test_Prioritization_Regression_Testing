diff --git a/lucene/src/java/org/apache/lucene/index/SegmentReader.java b/lucene/src/java/org/apache/lucene/index/SegmentReader.java
index 1cb3781..ac36827 100644
--- a/lucene/src/java/org/apache/lucene/index/SegmentReader.java
+++ b/lucene/src/java/org/apache/lucene/index/SegmentReader.java
@@ -22,22 +22,23 @@ import java.util.ArrayList;
 import java.util.Collection;
 import java.util.HashMap;
 import java.util.HashSet;
+
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
-import java.util.concurrent.atomic.AtomicInteger;
 
+import java.util.concurrent.atomic.AtomicInteger;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.FieldSelector;
-import org.apache.lucene.index.codecs.FieldsProducer;
 import org.apache.lucene.store.BufferedIndexInput;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IndexInput;
 import org.apache.lucene.store.IndexOutput;
 import org.apache.lucene.util.BitVector;
 import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.CloseableThreadLocal;
+import org.apache.lucene.index.codecs.FieldsProducer;
+import org.apache.lucene.util.BytesRef;
 
 /**
  * @lucene.experimental
@@ -119,7 +120,7 @@ public class SegmentReader extends IndexReader implements Cloneable {
         }
         cfsDir = dir0;
 
-        fieldInfos = si.getFieldInfos();
+        fieldInfos = new FieldInfos(cfsDir, IndexFileNames.segmentFileName(segment, "", IndexFileNames.FIELD_INFOS_EXTENSION));
         
         this.termsIndexDivisor = termsIndexDivisor;
         
@@ -597,12 +598,12 @@ public class SegmentReader extends IndexReader implements Cloneable {
                                   && (!si.hasDeletions() || this.si.getDelFileName().equals(si.getDelFileName()));
     boolean normsUpToDate = true;
     
-    Set<Integer> fieldNormsChanged = new HashSet<Integer>();
-    for (FieldInfo fi : core.fieldInfos) {
-      int fieldNumber = fi.number;
-      if (!this.si.getNormFileName(fieldNumber).equals(si.getNormFileName(fieldNumber))) {
+    boolean[] fieldNormsChanged = new boolean[core.fieldInfos.size()];
+    final int fieldCount = core.fieldInfos.size();
+    for (int i = 0; i < fieldCount; i++) {
+      if (!this.si.getNormFileName(i).equals(si.getNormFileName(i))) {
         normsUpToDate = false;
-        fieldNormsChanged.add(fieldNumber);
+        fieldNormsChanged[i] = true;
       }
     }
 
@@ -658,10 +659,11 @@ public class SegmentReader extends IndexReader implements Cloneable {
       clone.norms = new HashMap<String,Norm>();
 
       // Clone norms
-      for (FieldInfo fi : core.fieldInfos) {
+      for (int i = 0; i < fieldNormsChanged.length; i++) {
+
         // Clone unchanged norms to the cloned reader
-        if (doClone || !fieldNormsChanged.contains(fi.number)) {
-          final String curField = fi.name;
+        if (doClone || !fieldNormsChanged[i]) {
+          final String curField = core.fieldInfos.fieldInfo(i).name;
           Norm norm = this.norms.get(curField);
           if (norm != null)
             clone.norms.put(curField, (Norm) norm.clone());
@@ -733,7 +735,7 @@ public class SegmentReader extends IndexReader implements Cloneable {
     }
 
     if (normsDirty) {               // re-write norms
-      si.initNormGen();
+      si.initNormGen(core.fieldInfos.size());
       for (final Norm norm : norms.values()) {
         if (norm.dirty) {
           norm.reWrite(si);
@@ -878,7 +880,8 @@ public class SegmentReader extends IndexReader implements Cloneable {
     ensureOpen();
 
     Set<String> fieldSet = new HashSet<String>();
-    for (FieldInfo fi : core.fieldInfos) {
+    for (int i = 0; i < core.fieldInfos.size(); i++) {
+      FieldInfo fi = core.fieldInfos.fieldInfo(i);
       if (fieldOption == IndexReader.FieldOption.ALL) {
         fieldSet.add(fi.name);
       }
@@ -956,7 +959,8 @@ public class SegmentReader extends IndexReader implements Cloneable {
   private void openNorms(Directory cfsDir, int readBufferSize) throws IOException {
     long nextNormSeek = SegmentMerger.NORMS_HEADER.length; //skip header (header unused for now)
     int maxDoc = maxDoc();
-    for (FieldInfo fi : core.fieldInfos) {
+    for (int i = 0; i < core.fieldInfos.size(); i++) {
+      FieldInfo fi = core.fieldInfos.fieldInfo(i);
       if (norms.containsKey(fi.name)) {
         // in case this SegmentReader is being re-opened, we might be able to
         // reuse some norm instances and skip loading them here
