diff --git a/lucene/src/test/org/apache/lucene/index/TestIndexWriter.java b/lucene/src/test/org/apache/lucene/index/TestIndexWriter.java
index e273b14..9a990e6 100644
--- a/lucene/src/test/org/apache/lucene/index/TestIndexWriter.java
+++ b/lucene/src/test/org/apache/lucene/index/TestIndexWriter.java
@@ -133,14 +133,14 @@ public class TestIndexWriter extends LuceneTestCase {
         dir.close();
     }
 
-    private void addDoc(IndexWriter writer) throws IOException
+    static void addDoc(IndexWriter writer) throws IOException
     {
         Document doc = new Document();
         doc.add(newField("content", "aaa", Field.Store.NO, Field.Index.ANALYZED));
         writer.addDocument(doc);
     }
 
-    private void addDocWithIndex(IndexWriter writer, int index) throws IOException
+    static void addDocWithIndex(IndexWriter writer, int index) throws IOException
     {
         Document doc = new Document();
         doc.add(newField("content", "aaa " + index, Field.Store.YES, Field.Index.ANALYZED));
@@ -165,142 +165,6 @@ public class TestIndexWriter extends LuceneTestCase {
       }
     }
 
-    public void testOptimizeMaxNumSegments() throws IOException {
-
-      MockDirectoryWrapper dir = newDirectory();
-
-      final Document doc = new Document();
-      doc.add(newField("content", "aaa", Field.Store.YES, Field.Index.ANALYZED));
-      final int incrMin = TEST_NIGHTLY ? 15 : 40;
-      for(int numDocs=10;numDocs<500;numDocs += _TestUtil.nextInt(random, incrMin, 5*incrMin)) {
-        LogDocMergePolicy ldmp = new LogDocMergePolicy();
-        ldmp.setMinMergeDocs(1);
-        ldmp.setMergeFactor(5);
-        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(
-          TEST_VERSION_CURRENT, new MockAnalyzer(random))
-          .setOpenMode(OpenMode.CREATE).setMaxBufferedDocs(2).setMergePolicy(
-              ldmp));
-        for(int j=0;j<numDocs;j++)
-          writer.addDocument(doc);
-        writer.close();
-
-        SegmentInfos sis = new SegmentInfos();
-        sis.read(dir);
-        final int segCount = sis.size();
-
-        ldmp = new LogDocMergePolicy();
-        ldmp.setMergeFactor(5);
-        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,
-          new MockAnalyzer(random)).setMergePolicy(ldmp));
-        writer.optimize(3);
-        writer.close();
-
-        sis = new SegmentInfos();
-        sis.read(dir);
-        final int optSegCount = sis.size();
-
-        if (segCount < 3)
-          assertEquals(segCount, optSegCount);
-        else
-          assertEquals(3, optSegCount);
-      }
-      dir.close();
-    }
-
-    public void testOptimizeMaxNumSegments2() throws IOException {
-      MockDirectoryWrapper dir = newDirectory();
-
-      final Document doc = new Document();
-      doc.add(newField("content", "aaa", Field.Store.YES, Field.Index.ANALYZED));
-
-      LogDocMergePolicy ldmp = new LogDocMergePolicy();
-      ldmp.setMinMergeDocs(1);
-      ldmp.setMergeFactor(4);
-      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(
-        TEST_VERSION_CURRENT, new MockAnalyzer(random))
-        .setMaxBufferedDocs(2).setMergePolicy(ldmp).setMergeScheduler(new ConcurrentMergeScheduler()));
-
-      for(int iter=0;iter<10;iter++) {
-        for(int i=0;i<19;i++)
-          writer.addDocument(doc);
-
-        writer.commit();
-        writer.waitForMerges();
-        writer.commit();
-
-        SegmentInfos sis = new SegmentInfos();
-        sis.read(dir);
-
-        final int segCount = sis.size();
-
-        writer.optimize(7);
-        writer.commit();
-        writer.waitForMerges();
-
-        sis = new SegmentInfos();
-        sis.read(dir);
-        final int optSegCount = sis.size();
-
-        if (segCount < 7)
-          assertEquals(segCount, optSegCount);
-        else
-          assertEquals(7, optSegCount);
-      }
-      writer.close();
-      dir.close();
-    }
-
-    /**
-     * Make sure optimize doesn't use any more than 1X
-     * starting index size as its temporary free space
-     * required.
-     */
-    public void testOptimizeTempSpaceUsage() throws IOException {
-
-      MockDirectoryWrapper dir = newDirectory();
-      IndexWriter writer  = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMaxBufferedDocs(10).setMergePolicy(newLogMergePolicy()));
-      if (VERBOSE) {
-        System.out.println("TEST: config1=" + writer.getConfig());
-      }
-
-      for(int j=0;j<500;j++) {
-        addDocWithIndex(writer, j);
-      }
-      final int termIndexInterval = writer.getConfig().getTermIndexInterval();
-      // force one extra segment w/ different doc store so
-      // we see the doc stores get merged
-      writer.commit();
-      addDocWithIndex(writer, 500);
-      writer.close();
-
-      if (VERBOSE) {
-        System.out.println("TEST: start disk usage");
-      }
-      long startDiskUsage = 0;
-      String[] files = dir.listAll();
-      for(int i=0;i<files.length;i++) {
-        startDiskUsage += dir.fileLength(files[i]);
-        if (VERBOSE) {
-          System.out.println(files[i] + ": " + dir.fileLength(files[i]));
-        }
-      }
-
-      dir.resetMaxUsedSizeInBytes();
-      dir.setTrackDiskUsage(true);
-
-      // Import to use same term index interval else a
-      // smaller one here could increase the disk usage and
-      // cause a false failure:
-      writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setTermIndexInterval(termIndexInterval).setMergePolicy(newLogMergePolicy()));
-      writer.setInfoStream(VERBOSE ? System.out : null);
-      writer.optimize();
-      writer.close();
-      long maxDiskUsage = dir.getMaxUsedSizeInBytes();
-      assertTrue("optimize used too much temporary space: starting usage was " + startDiskUsage + " bytes; max temp usage was " + maxDiskUsage + " but should have been " + (4*startDiskUsage) + " (= 4X starting usage)",
-                 maxDiskUsage <= 4*startDiskUsage);
-      dir.close();
-    }
-
     static String arrayToString(String[] l) {
       String s = "";
       for(int i=0;i<l.length;i++) {
@@ -361,277 +225,7 @@ public class TestIndexWriter extends LuceneTestCase {
         dir.close();
     }
 
-    /*
-     * Simple test for "commit on close": open writer then
-     * add a bunch of docs, making sure reader does not see
-     * these docs until writer is closed.
-     */
-    public void testCommitOnClose() throws IOException {
-        Directory dir = newDirectory();
-        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));
-        for (int i = 0; i < 14; i++) {
-          addDoc(writer);
-        }
-        writer.close();
-
-        Term searchTerm = new Term("content", "aaa");
-        IndexSearcher searcher = new IndexSearcher(dir, false);
-        ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;
-        assertEquals("first number of hits", 14, hits.length);
-        searcher.close();
-
-        IndexReader reader = IndexReader.open(dir, true);
-
-        writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));
-        for(int i=0;i<3;i++) {
-          for(int j=0;j<11;j++) {
-            addDoc(writer);
-          }
-          searcher = new IndexSearcher(dir, false);
-          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;
-          assertEquals("reader incorrectly sees changes from writer", 14, hits.length);
-          searcher.close();
-          assertTrue("reader should have still been current", reader.isCurrent());
-        }
-
-        // Now, close the writer:
-        writer.close();
-        assertFalse("reader should not be current now", reader.isCurrent());
-
-        searcher = new IndexSearcher(dir, false);
-        hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;
-        assertEquals("reader did not see changes after writer was closed", 47, hits.length);
-        searcher.close();
-        reader.close();
-        dir.close();
-    }
-
-    /*
-     * Simple test for "commit on close": open writer, then
-     * add a bunch of docs, making sure reader does not see
-     * them until writer has closed.  Then instead of
-     * closing the writer, call abort and verify reader sees
-     * nothing was added.  Then verify we can open the index
-     * and add docs to it.
-     */
-    public void testCommitOnCloseAbort() throws IOException {
-      MockDirectoryWrapper dir = newDirectory();
-      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMaxBufferedDocs(10));
-      for (int i = 0; i < 14; i++) {
-        addDoc(writer);
-      }
-      writer.close();
-
-      Term searchTerm = new Term("content", "aaa");
-      IndexSearcher searcher = new IndexSearcher(dir, false);
-      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;
-      assertEquals("first number of hits", 14, hits.length);
-      searcher.close();
-
-      writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random))
-        .setOpenMode(OpenMode.APPEND).setMaxBufferedDocs(10));
-      for(int j=0;j<17;j++) {
-        addDoc(writer);
-      }
-      // Delete all docs:
-      writer.deleteDocuments(searchTerm);
-
-      searcher = new IndexSearcher(dir, false);
-      hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;
-      assertEquals("reader incorrectly sees changes from writer", 14, hits.length);
-      searcher.close();
-
-      // Now, close the writer:
-      writer.rollback();
-
-      assertNoUnreferencedFiles(dir, "unreferenced files remain after rollback()");
-
-      searcher = new IndexSearcher(dir, false);
-      hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;
-      assertEquals("saw changes after writer.abort", 14, hits.length);
-      searcher.close();
-
-      // Now make sure we can re-open the index, add docs,
-      // and all is good:
-      writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random))
-        .setOpenMode(OpenMode.APPEND).setMaxBufferedDocs(10));
-
-      // On abort, writer in fact may write to the same
-      // segments_N file:
-      dir.setPreventDoubleWrite(false);
-
-      for(int i=0;i<12;i++) {
-        for(int j=0;j<17;j++) {
-          addDoc(writer);
-        }
-        searcher = new IndexSearcher(dir, false);
-        hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;
-        assertEquals("reader incorrectly sees changes from writer", 14, hits.length);
-        searcher.close();
-      }
-
-      writer.close();
-      searcher = new IndexSearcher(dir, false);
-      hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;
-      assertEquals("didn't see changes after close", 218, hits.length);
-      searcher.close();
-
-      dir.close();
-    }
-
-    /*
-     * Verify that a writer with "commit on close" indeed
-     * cleans up the temp segments created after opening
-     * that are not referenced by the starting segments
-     * file.  We check this by using MockDirectoryWrapper to
-     * measure max temp disk space used.
-     */
-    public void testCommitOnCloseDiskUsage() throws IOException {
-      MockDirectoryWrapper dir = newDirectory();
-      Analyzer analyzer;
-      if (random.nextBoolean()) {
-        // no payloads
-       analyzer = new Analyzer() {
-          @Override
-          public TokenStream tokenStream(String fieldName, Reader reader) {
-            return new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);
-          }
-        };
-      } else {
-        // fixed length payloads
-        final int length = random.nextInt(200);
-        analyzer = new Analyzer() {
-          @Override
-          public TokenStream tokenStream(String fieldName, Reader reader) {
-            return new MockFixedLengthPayloadFilter(random,
-                new MockTokenizer(reader, MockTokenizer.WHITESPACE, true),
-                length);
-          }
-        };
-      }
-      
-      IndexWriter writer  = new IndexWriter(
-          dir,
-          newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).
-              setMaxBufferedDocs(10).
-              setReaderPooling(false).
-              setMergePolicy(newLogMergePolicy(10))
-      );
-      for(int j=0;j<30;j++) {
-        addDocWithIndex(writer, j);
-      }
-      writer.close();
-      dir.resetMaxUsedSizeInBytes();
-
-      dir.setTrackDiskUsage(true);
-      long startDiskUsage = dir.getMaxUsedSizeInBytes();
-      writer = new IndexWriter(
-          dir,
-          newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)
-              .setOpenMode(OpenMode.APPEND).
-              setMaxBufferedDocs(10).
-              setMergeScheduler(new SerialMergeScheduler()).
-              setReaderPooling(false).
-              setMergePolicy(newLogMergePolicy(10))
-
-      );
-      for(int j=0;j<1470;j++) {
-        addDocWithIndex(writer, j);
-      }
-      long midDiskUsage = dir.getMaxUsedSizeInBytes();
-      dir.resetMaxUsedSizeInBytes();
-      writer.optimize();
-      writer.close();
-
-      IndexReader.open(dir, true).close();
-
-      long endDiskUsage = dir.getMaxUsedSizeInBytes();
-
-      // Ending index is 50X as large as starting index; due
-      // to 3X disk usage normally we allow 150X max
-      // transient usage.  If something is wrong w/ deleter
-      // and it doesn't delete intermediate segments then it
-      // will exceed this 150X:
-      // System.out.println("start " + startDiskUsage + "; mid " + midDiskUsage + ";end " + endDiskUsage);
-      assertTrue("writer used too much space while adding documents: mid=" + midDiskUsage + " start=" + startDiskUsage + " end=" + endDiskUsage + " max=" + (startDiskUsage*150),
-                 midDiskUsage < 150*startDiskUsage);
-      assertTrue("writer used too much space after close: endDiskUsage=" + endDiskUsage + " startDiskUsage=" + startDiskUsage + " max=" + (startDiskUsage*150),
-                 endDiskUsage < 150*startDiskUsage);
-      dir.close();
-    }
-
-
-    /*
-     * Verify that calling optimize when writer is open for
-     * "commit on close" works correctly both for rollback()
-     * and close().
-     */
-    public void testCommitOnCloseOptimize() throws IOException {
-      MockDirectoryWrapper dir = newDirectory();
-      // Must disable throwing exc on double-write: this
-      // test uses IW.rollback which easily results in
-      // writing to same file more than once
-      dir.setPreventDoubleWrite(false);
-      IndexWriter writer = new IndexWriter(
-          dir,
-          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).
-              setMaxBufferedDocs(10).
-              setMergePolicy(newLogMergePolicy(10))
-      );
-      for(int j=0;j<17;j++) {
-        addDocWithIndex(writer, j);
-      }
-      writer.close();
-
-      writer  = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));
-      writer.optimize();
-
-      if (VERBOSE) {
-        writer.setInfoStream(System.out);
-      }
-
-      // Open a reader before closing (commiting) the writer:
-      IndexReader reader = IndexReader.open(dir, true);
-
-      // Reader should see index as unoptimized at this
-      // point:
-      assertFalse("Reader incorrectly sees that the index is optimized", reader.isOptimized());
-      reader.close();
-
-      // Abort the writer:
-      writer.rollback();
-      assertNoUnreferencedFiles(dir, "aborted writer after optimize");
-
-      // Open a reader after aborting writer:
-      reader = IndexReader.open(dir, true);
-
-      // Reader should still see index as unoptimized:
-      assertFalse("Reader incorrectly sees that the index is optimized", reader.isOptimized());
-      reader.close();
-
-      if (VERBOSE) {
-        System.out.println("TEST: do real optimize");
-      }
-      writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));
-      if (VERBOSE) {
-        writer.setInfoStream(System.out);
-      }
-      writer.optimize();
-      writer.close();
-
-      if (VERBOSE) {
-        System.out.println("TEST: writer closed");
-      }
-      assertNoUnreferencedFiles(dir, "aborted writer after optimize");
-
-      // Open a reader after aborting writer:
-      reader = IndexReader.open(dir, true);
 
-      // Reader should still see index as unoptimized:
-      assertTrue("Reader incorrectly sees that the index is unoptimized", reader.isOptimized());
-      reader.close();
-      dir.close();
-    }
 
     public void testIndexNoDocuments() throws IOException {
       MockDirectoryWrapper dir = newDirectory();
@@ -1027,57 +621,13 @@ public class TestIndexWriter extends LuceneTestCase {
       }
       writer.addDocument(new Document());
       writer.close();
-      _TestUtil.checkIndex(dir);
       IndexReader reader = IndexReader.open(dir, true);
       assertEquals(2, reader.numDocs());
       reader.close();
       dir.close();
     }
 
-    // Test calling optimize(false) whereby optimize is kicked
-    // off but we don't wait for it to finish (but
-    // writer.close()) does wait
-    public void testBackgroundOptimize() throws IOException {
-
-      Directory dir = newDirectory();
-      for(int pass=0;pass<2;pass++) {
-        IndexWriter writer = new IndexWriter(
-            dir,
-            newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).
-                setOpenMode(OpenMode.CREATE).
-                setMaxBufferedDocs(2).
-                setMergePolicy(newLogMergePolicy(51))
-        );
-        Document doc = new Document();
-        doc.add(newField("field", "aaa", Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));
-        for(int i=0;i<100;i++)
-          writer.addDocument(doc);
-        writer.optimize(false);
-
-        if (0 == pass) {
-          writer.close();
-          IndexReader reader = IndexReader.open(dir, true);
-          assertTrue(reader.isOptimized());
-          reader.close();
-        } else {
-          // Get another segment to flush so we can verify it is
-          // NOT included in the optimization
-          writer.addDocument(doc);
-          writer.addDocument(doc);
-          writer.close();
-
-          IndexReader reader = IndexReader.open(dir, true);
-          assertTrue(!reader.isOptimized());
-          reader.close();
 
-          SegmentInfos infos = new SegmentInfos();
-          infos.read(dir);
-          assertEquals(2, infos.size());
-        }
-      }
-
-      dir.close();
-    }
 
   /**
    * Test that no NullPointerException will be raised,
@@ -1121,51 +671,6 @@ public class TestIndexWriter extends LuceneTestCase {
     }
   }
 
-  // Just intercepts all merges & verifies that we are never
-  // merging a segment with >= 20 (maxMergeDocs) docs
-  private class MyMergeScheduler extends MergeScheduler {
-    @Override
-    synchronized public void merge(IndexWriter writer)
-      throws CorruptIndexException, IOException {
-
-      while(true) {
-        MergePolicy.OneMerge merge = writer.getNextMerge();
-        if (merge == null) {
-          break;
-        }
-        for(int i=0;i<merge.segments.size();i++) {
-          assert merge.segments.get(i).docCount < 20;
-        }
-        writer.merge(merge);
-      }
-    }
-
-    @Override
-    public void close() {}
-  }
-
-  // LUCENE-1013
-  public void testSetMaxMergeDocs() throws IOException {
-    Directory dir = newDirectory();
-    IndexWriterConfig conf = newIndexWriterConfig(
-        TEST_VERSION_CURRENT, new MockAnalyzer(random))
-      .setMergeScheduler(new MyMergeScheduler()).setMaxBufferedDocs(2).setMergePolicy(newLogMergePolicy());
-    LogMergePolicy lmp = (LogMergePolicy) conf.getMergePolicy();
-    lmp.setMaxMergeDocs(20);
-    lmp.setMergeFactor(2);
-    IndexWriter iw = new IndexWriter(dir, conf);
-    iw.setInfoStream(VERBOSE ? System.out : null);
-    Document document = new Document();
-    document.add(newField("tvtest", "a b c", Field.Store.NO, Field.Index.ANALYZED,
-                           Field.TermVector.YES));
-    for(int i=0;i<177;i++)
-      iw.addDocument(document);
-    iw.close();
-    dir.close();
-  }
-
-
-
   public void testVariableSchema() throws Exception {
     Directory dir = newDirectory();
     int delID = 0;
@@ -1339,184 +844,7 @@ public class TestIndexWriter extends LuceneTestCase {
     dir.close();
   }
 
-  // LUCENE-1044: test writer.commit() when ac=false
-  public void testForceCommit() throws IOException {
-    Directory dir = newDirectory();
 
-    IndexWriter writer = new IndexWriter(
-        dir,
-        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).
-            setMaxBufferedDocs(2).
-            setMergePolicy(newLogMergePolicy(5))
-    );
-    writer.commit();
-
-    for (int i = 0; i < 23; i++)
-      addDoc(writer);
-
-    IndexReader reader = IndexReader.open(dir, true);
-    assertEquals(0, reader.numDocs());
-    writer.commit();
-    IndexReader reader2 = reader.reopen();
-    assertEquals(0, reader.numDocs());
-    assertEquals(23, reader2.numDocs());
-    reader.close();
-
-    for (int i = 0; i < 17; i++)
-      addDoc(writer);
-    assertEquals(23, reader2.numDocs());
-    reader2.close();
-    reader = IndexReader.open(dir, true);
-    assertEquals(23, reader.numDocs());
-    reader.close();
-    writer.commit();
-
-    reader = IndexReader.open(dir, true);
-    assertEquals(40, reader.numDocs());
-    reader.close();
-    writer.close();
-    dir.close();
-  }
-
-  // LUCENE-325: test expungeDeletes, when 2 singular merges
-  // are required
-  public void testExpungeDeletes() throws IOException {
-    Directory dir = newDirectory();
-    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(
-        TEST_VERSION_CURRENT, new MockAnalyzer(random))
-        .setMaxBufferedDocs(2).setRAMBufferSizeMB(
-                                                  IndexWriterConfig.DISABLE_AUTO_FLUSH));
-    writer.setInfoStream(VERBOSE ? System.out : null);
-    Document document = new Document();
-
-    document = new Document();
-    Field storedField = newField("stored", "stored", Field.Store.YES,
-                                  Field.Index.NO);
-    document.add(storedField);
-    Field termVectorField = newField("termVector", "termVector",
-                                      Field.Store.NO, Field.Index.NOT_ANALYZED,
-                                      Field.TermVector.WITH_POSITIONS_OFFSETS);
-    document.add(termVectorField);
-    for(int i=0;i<10;i++)
-      writer.addDocument(document);
-    writer.close();
-
-    IndexReader ir = IndexReader.open(dir, false);
-    assertEquals(10, ir.maxDoc());
-    assertEquals(10, ir.numDocs());
-    ir.deleteDocument(0);
-    ir.deleteDocument(7);
-    assertEquals(8, ir.numDocs());
-    ir.close();
-
-    writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));
-    assertEquals(8, writer.numDocs());
-    assertEquals(10, writer.maxDoc());
-    writer.expungeDeletes();
-    assertEquals(8, writer.numDocs());
-    writer.close();
-    ir = IndexReader.open(dir, true);
-    assertEquals(8, ir.maxDoc());
-    assertEquals(8, ir.numDocs());
-    ir.close();
-    dir.close();
-  }
-
-  // LUCENE-325: test expungeDeletes, when many adjacent merges are required
-  public void testExpungeDeletes2() throws IOException {
-    Directory dir = newDirectory();
-    IndexWriter writer = new IndexWriter(
-        dir,
-        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).
-            setMaxBufferedDocs(2).
-            setRAMBufferSizeMB(IndexWriterConfig.DISABLE_AUTO_FLUSH).
-            setMergePolicy(newLogMergePolicy(50))
-    );
-
-    Document document = new Document();
-
-    document = new Document();
-    Field storedField = newField("stored", "stored", Store.YES,
-                                  Index.NO);
-    document.add(storedField);
-    Field termVectorField = newField("termVector", "termVector",
-                                      Store.NO, Index.NOT_ANALYZED,
-                                      TermVector.WITH_POSITIONS_OFFSETS);
-    document.add(termVectorField);
-    for(int i=0;i<98;i++)
-      writer.addDocument(document);
-    writer.close();
-
-    IndexReader ir = IndexReader.open(dir, false);
-    assertEquals(98, ir.maxDoc());
-    assertEquals(98, ir.numDocs());
-    for(int i=0;i<98;i+=2)
-      ir.deleteDocument(i);
-    assertEquals(49, ir.numDocs());
-    ir.close();
-
-    writer = new IndexWriter(
-        dir,
-        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).
-            setMergePolicy(newLogMergePolicy(3))
-    );
-    assertEquals(49, writer.numDocs());
-    writer.expungeDeletes();
-    writer.close();
-    ir = IndexReader.open(dir, true);
-    assertEquals(49, ir.maxDoc());
-    assertEquals(49, ir.numDocs());
-    ir.close();
-    dir.close();
-  }
-
-  // LUCENE-325: test expungeDeletes without waiting, when
-  // many adjacent merges are required
-  public void testExpungeDeletes3() throws IOException {
-    Directory dir = newDirectory();
-    IndexWriter writer = new IndexWriter(
-        dir,
-        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).
-            setMaxBufferedDocs(2).
-            setRAMBufferSizeMB(IndexWriterConfig.DISABLE_AUTO_FLUSH).
-            setMergePolicy(newLogMergePolicy(50))
-    );
-
-    Document document = new Document();
-
-    document = new Document();
-    Field storedField = newField("stored", "stored", Field.Store.YES,
-                                  Field.Index.NO);
-    document.add(storedField);
-    Field termVectorField = newField("termVector", "termVector",
-                                      Field.Store.NO, Field.Index.NOT_ANALYZED,
-                                      Field.TermVector.WITH_POSITIONS_OFFSETS);
-    document.add(termVectorField);
-    for(int i=0;i<98;i++)
-      writer.addDocument(document);
-    writer.close();
-
-    IndexReader ir = IndexReader.open(dir, false);
-    assertEquals(98, ir.maxDoc());
-    assertEquals(98, ir.numDocs());
-    for(int i=0;i<98;i+=2)
-      ir.deleteDocument(i);
-    assertEquals(49, ir.numDocs());
-    ir.close();
-
-    writer = new IndexWriter(
-        dir,
-        newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).
-            setMergePolicy(newLogMergePolicy(3))
-    );
-    writer.expungeDeletes(false);
-    writer.close();
-    ir = IndexReader.open(dir, true);
-    assertEquals(49, ir.maxDoc());
-    assertEquals(49, ir.numDocs());
-    ir.close();
-    dir.close();
-  }
 
   // LUCENE-1179
   public void testEmptyFieldName() throws IOException {
@@ -1539,208 +867,43 @@ public class TestIndexWriter extends LuceneTestCase {
 
     boolean afterWasCalled;
     boolean beforeWasCalled;
-
-    @Override
-    public void doAfterFlush() {
-      afterWasCalled = true;
-    }
-
-    @Override
-    protected void doBeforeFlush() throws IOException {
-      beforeWasCalled = true;
-    }
-  }
-
-
-  // LUCENE-1222
-  public void testDoBeforeAfterFlush() throws IOException {
-    Directory dir = newDirectory();
-    MockIndexWriter w = new MockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));
-    Document doc = new Document();
-    doc.add(newField("field", "a field", Field.Store.YES,
-                      Field.Index.ANALYZED));
-    w.addDocument(doc);
-    w.commit();
-    assertTrue(w.beforeWasCalled);
-    assertTrue(w.afterWasCalled);
-    w.beforeWasCalled = false;
-    w.afterWasCalled = false;
-    w.deleteDocuments(new Term("field", "field"));
-    w.commit();
-    assertTrue(w.beforeWasCalled);
-    assertTrue(w.afterWasCalled);
-    w.close();
-
-    IndexReader ir = IndexReader.open(dir, true);
-    assertEquals(0, ir.numDocs());
-    ir.close();
-
-    dir.close();
-  }
-
-
-
-  final String[] utf8Data = new String[] {
-    // unpaired low surrogate
-    "ab\udc17cd", "ab\ufffdcd",
-    "\udc17abcd", "\ufffdabcd",
-    "\udc17", "\ufffd",
-    "ab\udc17\udc17cd", "ab\ufffd\ufffdcd",
-    "\udc17\udc17abcd", "\ufffd\ufffdabcd",
-    "\udc17\udc17", "\ufffd\ufffd",
-
-    // unpaired high surrogate
-    "ab\ud917cd", "ab\ufffdcd",
-    "\ud917abcd", "\ufffdabcd",
-    "\ud917", "\ufffd",
-    "ab\ud917\ud917cd", "ab\ufffd\ufffdcd",
-    "\ud917\ud917abcd", "\ufffd\ufffdabcd",
-    "\ud917\ud917", "\ufffd\ufffd",
-
-    // backwards surrogates
-    "ab\udc17\ud917cd", "ab\ufffd\ufffdcd",
-    "\udc17\ud917abcd", "\ufffd\ufffdabcd",
-    "\udc17\ud917", "\ufffd\ufffd",
-    "ab\udc17\ud917\udc17\ud917cd", "ab\ufffd\ud917\udc17\ufffdcd",
-    "\udc17\ud917\udc17\ud917abcd", "\ufffd\ud917\udc17\ufffdabcd",
-    "\udc17\ud917\udc17\ud917", "\ufffd\ud917\udc17\ufffd"
-  };
-
-  // LUCENE-510
-  public void testInvalidUTF16() throws Throwable {
-    Directory dir = newDirectory();
-    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new StringSplitAnalyzer()));
-    Document doc = new Document();
-
-    final int count = utf8Data.length/2;
-    for(int i=0;i<count;i++)
-      doc.add(newField("f" + i, utf8Data[2*i], Field.Store.YES, Field.Index.ANALYZED));
-    w.addDocument(doc);
-    w.close();
-
-    IndexReader ir = IndexReader.open(dir, true);
-    Document doc2 = ir.document(0);
-    for(int i=0;i<count;i++) {
-      assertEquals("field " + i + " was not indexed correctly", 1, ir.docFreq(new Term("f"+i, utf8Data[2*i+1])));
-      assertEquals("field " + i + " is incorrect", utf8Data[2*i+1], doc2.getField("f"+i).stringValue());
-    }
-    ir.close();
-    dir.close();
-  }
-
-  // LUCENE-510
-  public void testAllUnicodeChars() throws Throwable {
-
-    BytesRef utf8 = new BytesRef(10);
-    CharsRef utf16 = new CharsRef(10);
-    char[] chars = new char[2];
-    for(int ch=0;ch<0x0010FFFF;ch++) {
-
-      if (ch == 0xd800)
-        // Skip invalid code points
-        ch = 0xe000;
-
-      int len = 0;
-      if (ch <= 0xffff) {
-        chars[len++] = (char) ch;
-      } else {
-        chars[len++] = (char) (((ch-0x0010000) >> 10) + UnicodeUtil.UNI_SUR_HIGH_START);
-        chars[len++] = (char) (((ch-0x0010000) & 0x3FFL) + UnicodeUtil.UNI_SUR_LOW_START);
-      }
-
-      UnicodeUtil.UTF16toUTF8(chars, 0, len, utf8);
-
-      String s1 = new String(chars, 0, len);
-      String s2 = new String(utf8.bytes, 0, utf8.length, "UTF-8");
-      assertEquals("codepoint " + ch, s1, s2);
-
-      UnicodeUtil.UTF8toUTF16(utf8.bytes, 0, utf8.length, utf16);
-      assertEquals("codepoint " + ch, s1, new String(utf16.chars, 0, utf16.length));
-
-      byte[] b = s1.getBytes("UTF-8");
-      assertEquals(utf8.length, b.length);
-      for(int j=0;j<utf8.length;j++)
-        assertEquals(utf8.bytes[j], b[j]);
-    }
-  }
-
-  private int nextInt(int lim) {
-    return random.nextInt(lim);
-  }
-
-  private int nextInt(int start, int end) {
-    return start + nextInt(end-start);
-  }
-
-  private boolean fillUnicode(char[] buffer, char[] expected, int offset, int count) {
-    final int len = offset + count;
-    boolean hasIllegal = false;
-
-    if (offset > 0 && buffer[offset] >= 0xdc00 && buffer[offset] < 0xe000)
-      // Don't start in the middle of a valid surrogate pair
-      offset--;
-
-    for(int i=offset;i<len;i++) {
-      int t = nextInt(6);
-      if (0 == t && i < len-1) {
-        // Make a surrogate pair
-        // High surrogate
-        expected[i] = buffer[i++] = (char) nextInt(0xd800, 0xdc00);
-        // Low surrogate
-        expected[i] = buffer[i] = (char) nextInt(0xdc00, 0xe000);
-      } else if (t <= 1)
-        expected[i] = buffer[i] = (char) nextInt(0x80);
-      else if (2 == t)
-        expected[i] = buffer[i] = (char) nextInt(0x80, 0x800);
-      else if (3 == t)
-        expected[i] = buffer[i] = (char) nextInt(0x800, 0xd800);
-      else if (4 == t)
-        expected[i] = buffer[i] = (char) nextInt(0xe000, 0xffff);
-      else if (5 == t && i < len-1) {
-        // Illegal unpaired surrogate
-        if (nextInt(10) == 7) {
-          if (random.nextBoolean())
-            buffer[i] = (char) nextInt(0xd800, 0xdc00);
-          else
-            buffer[i] = (char) nextInt(0xdc00, 0xe000);
-          expected[i++] = 0xfffd;
-          expected[i] = buffer[i] = (char) nextInt(0x800, 0xd800);
-          hasIllegal = true;
-        } else
-          expected[i] = buffer[i] = (char) nextInt(0x800, 0xd800);
-      } else {
-        expected[i] = buffer[i] = ' ';
-      }
+
+    @Override
+    public void doAfterFlush() {
+      afterWasCalled = true;
     }
 
-    return hasIllegal;
+    @Override
+    protected void doBeforeFlush() throws IOException {
+      beforeWasCalled = true;
+    }
   }
 
-  // LUCENE-510
-  public void testRandomUnicodeStrings() throws Throwable {
-    char[] buffer = new char[20];
-    char[] expected = new char[20];
 
-    BytesRef utf8 = new BytesRef(20);
-    CharsRef utf16 = new CharsRef(20);
-
-    int num = 100000 * RANDOM_MULTIPLIER;
-    for (int iter = 0; iter < num; iter++) {
-      boolean hasIllegal = fillUnicode(buffer, expected, 0, 20);
+  // LUCENE-1222
+  public void testDoBeforeAfterFlush() throws IOException {
+    Directory dir = newDirectory();
+    MockIndexWriter w = new MockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));
+    Document doc = new Document();
+    doc.add(newField("field", "a field", Field.Store.YES,
+                      Field.Index.ANALYZED));
+    w.addDocument(doc);
+    w.commit();
+    assertTrue(w.beforeWasCalled);
+    assertTrue(w.afterWasCalled);
+    w.beforeWasCalled = false;
+    w.afterWasCalled = false;
+    w.deleteDocuments(new Term("field", "field"));
+    w.commit();
+    assertTrue(w.beforeWasCalled);
+    assertTrue(w.afterWasCalled);
+    w.close();
 
-      UnicodeUtil.UTF16toUTF8(buffer, 0, 20, utf8);
-      if (!hasIllegal) {
-        byte[] b = new String(buffer, 0, 20).getBytes("UTF-8");
-        assertEquals(b.length, utf8.length);
-        for(int i=0;i<b.length;i++)
-          assertEquals(b[i], utf8.bytes[i]);
-      }
+    IndexReader ir = IndexReader.open(dir, true);
+    assertEquals(0, ir.numDocs());
+    ir.close();
 
-      UnicodeUtil.UTF8toUTF16(utf8.bytes, 0, utf8.length, utf16);
-      assertEquals(utf16.length, 20);
-      for(int i=0;i<20;i++)
-        assertEquals(expected[i], utf16.chars[i]);
-    }
+    dir.close();
   }
 
   // LUCENE-1255
@@ -1792,137 +955,11 @@ public class TestIndexWriter extends LuceneTestCase {
     assertEquals(0, tps.nextPosition());
     w.close();
 
-    _TestUtil.checkIndex(dir);
     s.close();
     dir.close();
   }
 
-  // LUCENE-1274: test writer.prepareCommit()
-  public void testPrepareCommit() throws IOException {
-    Directory dir = newDirectory();
-
-    IndexWriter writer = new IndexWriter(
-        dir,
-        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).
-            setMaxBufferedDocs(2).
-            setMergePolicy(newLogMergePolicy(5))
-    );
-    writer.commit();
-
-    for (int i = 0; i < 23; i++)
-      addDoc(writer);
-
-    IndexReader reader = IndexReader.open(dir, true);
-    assertEquals(0, reader.numDocs());
-
-    writer.prepareCommit();
-
-    IndexReader reader2 = IndexReader.open(dir, true);
-    assertEquals(0, reader2.numDocs());
-
-    writer.commit();
-
-    IndexReader reader3 = reader.reopen();
-    assertEquals(0, reader.numDocs());
-    assertEquals(0, reader2.numDocs());
-    assertEquals(23, reader3.numDocs());
-    reader.close();
-    reader2.close();
-
-    for (int i = 0; i < 17; i++)
-      addDoc(writer);
-
-    assertEquals(23, reader3.numDocs());
-    reader3.close();
-    reader = IndexReader.open(dir, true);
-    assertEquals(23, reader.numDocs());
-    reader.close();
-
-    writer.prepareCommit();
-
-    reader = IndexReader.open(dir, true);
-    assertEquals(23, reader.numDocs());
-    reader.close();
-
-    writer.commit();
-    reader = IndexReader.open(dir, true);
-    assertEquals(40, reader.numDocs());
-    reader.close();
-    writer.close();
-    dir.close();
-  }
-
-  // LUCENE-1274: test writer.prepareCommit()
-  public void testPrepareCommitRollback() throws IOException {
-    MockDirectoryWrapper dir = newDirectory();
-    dir.setPreventDoubleWrite(false);
-
-    IndexWriter writer = new IndexWriter(
-        dir,
-        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).
-            setMaxBufferedDocs(2).
-            setMergePolicy(newLogMergePolicy(5))
-    );
-    writer.commit();
-
-    for (int i = 0; i < 23; i++)
-      addDoc(writer);
-
-    IndexReader reader = IndexReader.open(dir, true);
-    assertEquals(0, reader.numDocs());
-
-    writer.prepareCommit();
-
-    IndexReader reader2 = IndexReader.open(dir, true);
-    assertEquals(0, reader2.numDocs());
-
-    writer.rollback();
-
-    IndexReader reader3 = reader.reopen();
-    assertEquals(0, reader.numDocs());
-    assertEquals(0, reader2.numDocs());
-    assertEquals(0, reader3.numDocs());
-    reader.close();
-    reader2.close();
-
-    writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));
-    for (int i = 0; i < 17; i++)
-      addDoc(writer);
-
-    assertEquals(0, reader3.numDocs());
-    reader3.close();
-    reader = IndexReader.open(dir, true);
-    assertEquals(0, reader.numDocs());
-    reader.close();
-
-    writer.prepareCommit();
-
-    reader = IndexReader.open(dir, true);
-    assertEquals(0, reader.numDocs());
-    reader.close();
-
-    writer.commit();
-    reader = IndexReader.open(dir, true);
-    assertEquals(17, reader.numDocs());
-    reader.close();
-    writer.close();
-    dir.close();
-  }
-
-  // LUCENE-1274
-  public void testPrepareCommitNoChanges() throws IOException {
-    Directory dir = newDirectory();
-
-    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));
-    writer.prepareCommit();
-    writer.commit();
-    writer.close();
 
-    IndexReader reader = IndexReader.open(dir, true);
-    assertEquals(0, reader.numDocs());
-    reader.close();
-    dir.close();
-  }
 
   // LUCENE-1219
   public void testBinaryFieldOffsetLength() throws IOException {
@@ -1954,45 +991,6 @@ public class TestIndexWriter extends LuceneTestCase {
     dir.close();
   }
 
-  // LUCENE-1382
-  public void testCommitUserData() throws IOException {
-    Directory dir = newDirectory();
-    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMaxBufferedDocs(2));
-    for(int j=0;j<17;j++)
-      addDoc(w);
-    w.close();
-
-    assertEquals(0, IndexReader.getCommitUserData(dir).size());
-
-    IndexReader r = IndexReader.open(dir, true);
-    // commit(Map) never called for this index
-    assertEquals(0, r.getCommitUserData().size());
-    r.close();
-
-    w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMaxBufferedDocs(2));
-    for(int j=0;j<17;j++)
-      addDoc(w);
-    Map<String,String> data = new HashMap<String,String>();
-    data.put("label", "test1");
-    w.commit(data);
-    w.close();
-
-    assertEquals("test1", IndexReader.getCommitUserData(dir).get("label"));
-
-    r = IndexReader.open(dir, true);
-    assertEquals("test1", r.getCommitUserData().get("label"));
-    r.close();
-
-    w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));
-    w.optimize();
-    w.close();
-
-    assertEquals("test1", IndexReader.getCommitUserData(dir).get("label"));
-
-    dir.close();
-  }
-
-
   // LUCENE-2529
   public void testPositionIncrementGapEmptyField() throws Exception {
     Directory dir = newDirectory();
@@ -2299,24 +1297,6 @@ public class TestIndexWriter extends LuceneTestCase {
     d.close();
   }
 
-  public void testEmbeddedFFFF() throws Throwable {
-
-    Directory d = newDirectory();
-    IndexWriter w = new IndexWriter(d, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));
-    Document doc = new Document();
-    doc.add(newField("field", "a a\uffffb", Field.Store.NO, Field.Index.ANALYZED));
-    w.addDocument(doc);
-    doc = new Document();
-    doc.add(newField("field", "a", Field.Store.NO, Field.Index.ANALYZED));
-    w.addDocument(doc);
-    IndexReader r = w.getReader();
-    assertEquals(1, r.docFreq(new Term("field", "a\uffffb")));
-    r.close();
-    w.close();
-    _TestUtil.checkIndex(d);
-    d.close();
-  }
-
   public void testNoDocsIndex() throws Throwable {
     Directory dir = newDirectory();
     IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(
@@ -2326,185 +1306,6 @@ public class TestIndexWriter extends LuceneTestCase {
     writer.addDocument(new Document());
     writer.close();
 
-    _TestUtil.checkIndex(dir);
-    dir.close();
-  }
-
-  // LUCENE-2095: make sure with multiple threads commit
-  // doesn't return until all changes are in fact in the
-  // index
-  public void testCommitThreadSafety() throws Throwable {
-    final int NUM_THREADS = 5;
-    final double RUN_SEC = 0.5;
-    final Directory dir = newDirectory();
-    final RandomIndexWriter w = new RandomIndexWriter(random, dir, newIndexWriterConfig(
-                                                                                        TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));
-    _TestUtil.reduceOpenFiles(w.w);
-    w.commit();
-    final AtomicBoolean failed = new AtomicBoolean();
-    Thread[] threads = new Thread[NUM_THREADS];
-    final long endTime = System.currentTimeMillis()+((long) (RUN_SEC*1000));
-    for(int i=0;i<NUM_THREADS;i++) {
-      final int finalI = i;
-      threads[i] = new Thread() {
-          @Override
-          public void run() {
-            try {
-              final Document doc = new Document();
-              IndexReader r = IndexReader.open(dir);
-              Field f = newField("f", "", Field.Store.NO, Field.Index.NOT_ANALYZED);
-              doc.add(f);
-              int count = 0;
-              do {
-                if (failed.get()) break;
-                for(int j=0;j<10;j++) {
-                  final String s = finalI + "_" + String.valueOf(count++);
-                  f.setValue(s);
-                  w.addDocument(doc);
-                  w.commit();
-                  IndexReader r2 = r.reopen();
-                  assertTrue(r2 != r);
-                  r.close();
-                  r = r2;
-                  assertEquals("term=f:" + s + "; r=" + r, 1, r.docFreq(new Term("f", s)));
-                }
-              } while(System.currentTimeMillis() < endTime);
-              r.close();
-            } catch (Throwable t) {
-              failed.set(true);
-              throw new RuntimeException(t);
-            }
-          }
-        };
-      threads[i].start();
-    }
-    for(int i=0;i<NUM_THREADS;i++) {
-      threads[i].join();
-    }
-    assertFalse(failed.get());
-    w.close();
-    dir.close();
-  }
-
-  // both start & end are inclusive
-  private final int getInt(Random r, int start, int end) {
-    return start + r.nextInt(1+end-start);
-  }
-
-  private void checkTermsOrder(IndexReader r, Set<String> allTerms, boolean isTop) throws IOException {
-    TermsEnum terms = MultiFields.getFields(r).terms("f").iterator();
-
-    BytesRef last = new BytesRef();
-
-    Set<String> seenTerms = new HashSet<String>();
-
-    while(true) {
-      final BytesRef term = terms.next();
-      if (term == null) {
-        break;
-      }
-
-      assertTrue(last.compareTo(term) < 0);
-      last.copy(term);
-
-      final String s = term.utf8ToString();
-      assertTrue("term " + termDesc(s) + " was not added to index (count=" + allTerms.size() + ")", allTerms.contains(s));
-      seenTerms.add(s);
-    }
-
-    if (isTop) {
-      assertTrue(allTerms.equals(seenTerms));
-    }
-
-    // Test seeking:
-    Iterator<String> it = seenTerms.iterator();
-    while(it.hasNext()) {
-      BytesRef tr = new BytesRef(it.next());
-      assertEquals("seek failed for term=" + termDesc(tr.utf8ToString()),
-                   TermsEnum.SeekStatus.FOUND,
-                   terms.seek(tr));
-    }
-  }
-
-  private final String asUnicodeChar(char c) {
-    return "U+" + Integer.toHexString(c);
-  }
-
-  private final String termDesc(String s) {
-    final String s0;
-    assertTrue(s.length() <= 2);
-    if (s.length() == 1) {
-      s0 = asUnicodeChar(s.charAt(0));
-    } else {
-      s0 = asUnicodeChar(s.charAt(0)) + "," + asUnicodeChar(s.charAt(1));
-    }
-    return s0;
-  }
-
-  // Make sure terms, including ones with surrogate pairs,
-  // sort in codepoint sort order by default
-  public void testTermUTF16SortOrder() throws Throwable {
-    Random rnd = random;
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(rnd, dir);
-    Document d = new Document();
-    // Single segment
-    Field f = newField("f", "", Field.Store.NO, Field.Index.NOT_ANALYZED);
-    d.add(f);
-    char[] chars = new char[2];
-    final Set<String> allTerms = new HashSet<String>();
-
-    int num = 200 * RANDOM_MULTIPLIER;
-    for (int i = 0; i < num; i++) {
-
-      final String s;
-      if (rnd.nextBoolean()) {
-        // Single char
-        if (rnd.nextBoolean()) {
-          // Above surrogates
-          chars[0] = (char) getInt(rnd, 1+UnicodeUtil.UNI_SUR_LOW_END, 0xffff);
-        } else {
-          // Below surrogates
-          chars[0] = (char) getInt(rnd, 0, UnicodeUtil.UNI_SUR_HIGH_START-1);
-        }
-        s = new String(chars, 0, 1);
-      } else {
-        // Surrogate pair
-        chars[0] = (char) getInt(rnd, UnicodeUtil.UNI_SUR_HIGH_START, UnicodeUtil.UNI_SUR_HIGH_END);
-        assertTrue(((int) chars[0]) >= UnicodeUtil.UNI_SUR_HIGH_START && ((int) chars[0]) <= UnicodeUtil.UNI_SUR_HIGH_END);
-        chars[1] = (char) getInt(rnd, UnicodeUtil.UNI_SUR_LOW_START, UnicodeUtil.UNI_SUR_LOW_END);
-        s = new String(chars, 0, 2);
-      }
-      allTerms.add(s);
-      f.setValue(s);
-
-      writer.addDocument(d);
-
-      if ((1+i) % 42 == 0) {
-        writer.commit();
-      }
-    }
-
-    IndexReader r = writer.getReader();
-
-    // Test each sub-segment
-    final IndexReader[] subs = r.getSequentialSubReaders();
-    for(int i=0;i<subs.length;i++) {
-      checkTermsOrder(subs[i], allTerms, false);
-    }
-    checkTermsOrder(r, allTerms, true);
-
-    // Test multi segment
-    r.close();
-
-    writer.optimize();
-
-    // Test optimized single segment
-    r = writer.getReader();
-    checkTermsOrder(r, allTerms, true);
-    r.close();
-
-    writer.close();
     dir.close();
   }
 
@@ -2638,67 +1439,6 @@ public class TestIndexWriter extends LuceneTestCase {
     dir.close();
   }
 
-  public void testIndexingThenDeleting() throws Exception {
-    final Random r = random;
-    Directory dir = newDirectory();
-    // note this test explicitly disables payloads
-    final Analyzer analyzer = new Analyzer() {
-      @Override
-      public TokenStream tokenStream(String fieldName, Reader reader) {
-        return new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);
-      }
-    };
-    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));
-    w.setInfoStream(VERBOSE ? System.out : null);
-    Document doc = new Document();
-    doc.add(newField("field", "go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20", Field.Store.NO, Field.Index.ANALYZED));
-    int num = TEST_NIGHTLY ? 6 * RANDOM_MULTIPLIER : 3 * RANDOM_MULTIPLIER;
-    for (int iter = 0; iter < num; iter++) {
-      int count = 0;
-
-      final boolean doIndexing = r.nextBoolean();
-      if (VERBOSE) {
-        System.out.println("TEST: iter doIndexing=" + doIndexing);
-      }
-      if (doIndexing) {
-        // Add docs until a flush is triggered
-        final int startFlushCount = w.getFlushCount();
-        while(w.getFlushCount() == startFlushCount) {
-          w.addDocument(doc);
-          count++;
-        }
-      } else {
-        // Delete docs until a flush is triggered
-        final int startFlushCount = w.getFlushCount();
-        while(w.getFlushCount() == startFlushCount) {
-          w.deleteDocuments(new Term("foo", ""+count));
-          count++;
-        }
-      }
-      assertTrue("flush happened too quickly during " + (doIndexing ? "indexing" : "deleting") + " count=" + count, count > 3000);
-    }
-    w.close();
-    dir.close();
-  }
-
-  public void testNoCommits() throws Exception {
-    // Tests that if we don't call commit(), the directory has 0 commits. This has
-    // changed since LUCENE-2386, where before IW would always commit on a fresh
-    // new index.
-    Directory dir = newDirectory();
-    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));
-    try {
-      IndexReader.listCommits(dir);
-      fail("listCommits should have thrown an exception over empty index");
-    } catch (IndexNotFoundException e) {
-      // that's expected !
-    }
-    // No changes still should generate a commit, because it's a new index.
-    writer.close();
-    assertEquals("expected 1 commits!", 1, IndexReader.listCommits(dir).size());
-    dir.close();
-  }
-
   public void testEmptyFSDirWithNoLock() throws Exception {
     // Tests that if FSDir is opened w/ a NoLockFactory (or SingleInstanceLF),
     // then IndexWriter ctor succeeds. Previously (LUCENE-2386) it failed
@@ -2775,82 +1515,6 @@ public class TestIndexWriter extends LuceneTestCase {
     dir.close();
   }
 
-  public void testFutureCommit() throws Exception {
-    Directory dir = newDirectory();
-
-    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setIndexDeletionPolicy(NoDeletionPolicy.INSTANCE));
-    Document doc = new Document();
-    w.addDocument(doc);
-
-    // commit to "first"
-    Map<String,String> commitData = new HashMap<String,String>();
-    commitData.put("tag", "first");
-    w.commit(commitData);
-
-    // commit to "second"
-    w.addDocument(doc);
-    commitData.put("tag", "second");
-    w.commit(commitData);
-    w.close();
-
-    // open "first" with IndexWriter
-    IndexCommit commit = null;
-    for(IndexCommit c : IndexReader.listCommits(dir)) {
-      if (c.getUserData().get("tag").equals("first")) {
-        commit = c;
-        break;
-      }
-    }
-
-    assertNotNull(commit);
-
-    w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setIndexDeletionPolicy(NoDeletionPolicy.INSTANCE).setIndexCommit(commit));
-
-    assertEquals(1, w.numDocs());
-
-    // commit IndexWriter to "third"
-    w.addDocument(doc);
-    commitData.put("tag", "third");
-    w.commit(commitData);
-    w.close();
-
-    // make sure "second" commit is still there
-    commit = null;
-    for(IndexCommit c : IndexReader.listCommits(dir)) {
-      if (c.getUserData().get("tag").equals("second")) {
-        commit = c;
-        break;
-      }
-    }
-
-    assertNotNull(commit);
-
-    IndexReader r = IndexReader.open(commit, true);
-    assertEquals(2, r.numDocs());
-    r.close();
-
-    // open "second", w/ writeable IndexReader & commit
-    r = IndexReader.open(commit, NoDeletionPolicy.INSTANCE, false);
-    assertEquals(2, r.numDocs());
-    r.deleteDocument(0);
-    r.deleteDocument(1);
-    commitData.put("tag", "fourth");
-    r.commit(commitData);
-    r.close();
-
-    // make sure "third" commit is still there
-    commit = null;
-    for(IndexCommit c : IndexReader.listCommits(dir)) {
-      if (c.getUserData().get("tag").equals("third")) {
-        commit = c;
-        break;
-      }
-    }
-    assertNotNull(commit);
-
-    dir.close();
-  }
-
   public void testRandomStoredFields() throws IOException {
     Directory dir = newDirectory();
     Random rand = random;
@@ -2980,39 +1644,7 @@ public class TestIndexWriter extends LuceneTestCase {
     dir.close();
   }
 
-  public void testDeleteAllSlowly() throws Exception {
-    final Directory dir = newDirectory();
-    RandomIndexWriter w = new RandomIndexWriter(random, dir);
-    final int NUM_DOCS = 1000 * RANDOM_MULTIPLIER;
-    final List<Integer> ids = new ArrayList<Integer>(NUM_DOCS);
-    for(int id=0;id<NUM_DOCS;id++) {
-      ids.add(id);
-    }
-    Collections.shuffle(ids, random);
-    for(int id : ids) {
-      Document doc = new Document();
-      doc.add(newField("id", ""+id, Field.Index.NOT_ANALYZED));
-      w.addDocument(doc);
-    }
-    Collections.shuffle(ids, random);
-    int upto = 0;
-    while(upto < ids.size()) {
-      final int left = ids.size() - upto;
-      final int inc = Math.min(left, _TestUtil.nextInt(random, 1, 20));
-      final int limit = upto + inc;
-      while(upto < limit) {
-        w.deleteDocuments(new Term("id", ""+ids.get(upto++)));
-      }
-      final IndexReader r = w.getReader();
-      assertEquals(NUM_DOCS - upto, r.numDocs());
-      r.close();
-    }
-
-    w.close();
-    dir.close();
-  }
-
-  private static class StringSplitAnalyzer extends Analyzer {
+  static final class StringSplitAnalyzer extends Analyzer {
     @Override
     public TokenStream tokenStream(String fieldName, Reader reader) {
       return new StringSplitTokenizer(reader);
