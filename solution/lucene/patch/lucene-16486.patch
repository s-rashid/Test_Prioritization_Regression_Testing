diff --git a/solr/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase.java b/solr/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase.java
index b87dcc3..4ec455b 100644
--- a/solr/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase.java
+++ b/solr/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase.java
@@ -20,10 +20,14 @@ package org.apache.solr.handler;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CharReader;
 import org.apache.lucene.analysis.CharStream;
-import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.tokenattributes.*;
 import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.index.Payload;
+import org.apache.lucene.util.Attribute;
+import org.apache.lucene.util.AttributeSource;
+import org.apache.lucene.util.AttributeReflector;
+import org.apache.lucene.util.SorterTemplate;
 import org.apache.solr.analysis.CharFilterFactory;
 import org.apache.solr.analysis.TokenFilterFactory;
 import org.apache.solr.analysis.TokenizerChain;
@@ -34,6 +38,9 @@ import org.apache.solr.common.SolrException;
 import org.apache.solr.request.SolrQueryRequest;
 import org.apache.solr.response.SolrQueryResponse;
 import org.apache.solr.schema.FieldType;
+import org.apache.solr.util.ByteUtils;
+
+import org.apache.noggit.CharArr;
 
 import java.io.IOException;
 import java.io.StringReader;
@@ -47,7 +54,7 @@ import java.util.*;
  */
 public abstract class AnalysisRequestHandlerBase extends RequestHandlerBase {
 
-  public static final Set<String> EMPTY_STRING_SET = Collections.emptySet();
+  public static final Set<BytesRef> EMPTY_BYTES_SET = Collections.emptySet();
 
   public void handleRequestBody(SolrQueryRequest req, SolrQueryResponse rsp) throws Exception {
     rsp.add("analysis", doAnalysis(req));
@@ -107,7 +114,7 @@ public abstract class AnalysisRequestHandlerBase extends RequestHandlerBase {
     }
 
     TokenStream tokenStream = tfac.create(tokenizerChain.charStream(new StringReader(value)));
-    List<Token> tokens = analyzeTokenStream(tokenStream);
+    List<AttributeSource> tokens = analyzeTokenStream(tokenStream);
 
     namedList.add(tokenStream.getClass().getName(), convertTokensToNamedLists(tokens, context));
 
@@ -115,7 +122,7 @@ public abstract class AnalysisRequestHandlerBase extends RequestHandlerBase {
 
     for (TokenFilterFactory tokenFilterFactory : filtfacs) {
       tokenStream = tokenFilterFactory.create(listBasedTokenStream);
-      List<Token> tokenList = analyzeTokenStream(tokenStream);
+      List<AttributeSource> tokenList = analyzeTokenStream(tokenStream);
       namedList.add(tokenStream.getClass().getName(), convertTokensToNamedLists(tokenList, context));
       listBasedTokenStream = new ListBasedTokenStream(tokenList);
     }
@@ -126,14 +133,24 @@ public abstract class AnalysisRequestHandlerBase extends RequestHandlerBase {
   /**
    * Analyzes the given text using the given analyzer and returns the produced tokens.
    *
-   * @param value    The value to analyze.
+   * @param query    The query to analyze.
    * @param analyzer The analyzer to use.
-   *
-   * @return The produces token list.
    */
-  protected List<Token> analyzeValue(String value, Analyzer analyzer) {
-    TokenStream tokenStream = analyzer.tokenStream("", new StringReader(value));
-    return analyzeTokenStream(tokenStream);
+  protected Set<BytesRef> getQueryTokenSet(String query, Analyzer analyzer) {
+    final Set<BytesRef> tokens = new HashSet<BytesRef>();
+    final TokenStream tokenStream = analyzer.tokenStream("", new StringReader(query));
+    final TermToBytesRefAttribute bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);
+    try {
+      tokenStream.reset();
+      while (tokenStream.incrementToken()) {
+        final BytesRef bytes = new BytesRef();
+        bytesAtt.toBytesRef(bytes);
+        tokens.add(bytes);
+      }
+    } catch (IOException ioe) {
+      throw new RuntimeException("Error occured while iterating over tokenstream", ioe);
+    }
+    return tokens;
   }
 
   /**
@@ -143,41 +160,17 @@ public abstract class AnalysisRequestHandlerBase extends RequestHandlerBase {
    *
    * @return List of tokens produced from the TokenStream
    */
-  private List<Token> analyzeTokenStream(TokenStream tokenStream) {
-    List<Token> tokens = new ArrayList<Token>();
-    
-    // TODO change this API to support custom attributes
-    CharTermAttribute termAtt = null;
-    TermToBytesRefAttribute bytesAtt = null;
-    if (tokenStream.hasAttribute(CharTermAttribute.class)) {
-      termAtt = tokenStream.getAttribute(CharTermAttribute.class);
-    } else if (tokenStream.hasAttribute(TermToBytesRefAttribute.class)) {
-      bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);
-    }
-    final OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);
-    final TypeAttribute typeAtt = tokenStream.addAttribute(TypeAttribute.class);
-    final PositionIncrementAttribute posIncAtt = tokenStream.addAttribute(PositionIncrementAttribute.class);
-    final FlagsAttribute flagsAtt = tokenStream.addAttribute(FlagsAttribute.class);
-    final PayloadAttribute payloadAtt = tokenStream.addAttribute(PayloadAttribute.class);
-    
+  private List<AttributeSource> analyzeTokenStream(TokenStream tokenStream) {
+    List<AttributeSource> tokens = new ArrayList<AttributeSource>();
+    // for backwards compatibility, add all "common" attributes
+    tokenStream.addAttribute(PositionIncrementAttribute.class);
+    tokenStream.addAttribute(OffsetAttribute.class);
+    tokenStream.addAttribute(TypeAttribute.class);
     final BytesRef bytes = new BytesRef();
     try {
+      tokenStream.reset();
       while (tokenStream.incrementToken()) {
-        Token token = new Token();
-        if (termAtt != null) {
-          token.setEmpty().append(termAtt);
-        }
-        if (bytesAtt != null) {
-          bytesAtt.toBytesRef(bytes);
-          // TODO: This is incorrect when numeric fields change in later lucene versions. It should use BytesRef directly!
-          token.setEmpty().append(bytes.utf8ToString());
-        }
-        token.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());
-        token.setType(typeAtt.type());
-        token.setFlags(flagsAtt.getFlags());
-        token.setPayload(payloadAtt.getPayload());
-        token.setPositionIncrement(posIncAtt.getPositionIncrement());
-        tokens.add((Token) token.clone());
+        tokens.add(tokenStream.cloneAttributes());
       }
     } catch (IOException ioe) {
       throw new RuntimeException("Error occured while iterating over tokenstream", ioe);
@@ -186,6 +179,13 @@ public abstract class AnalysisRequestHandlerBase extends RequestHandlerBase {
     return tokens;
   }
 
+  // a static mapping of the reflected attribute keys to the names used in Solr 1.4
+  static Map<String,String> ATTRIBUTE_MAPPING = Collections.unmodifiableMap(new HashMap<String,String>() {{
+    put(OffsetAttribute.class.getName() + "#startOffset", "start");
+    put(OffsetAttribute.class.getName() + "#endOffset", "end");
+    put(TypeAttribute.class.getName() + "#type", "type");
+  }});
+
   /**
    * Converts the list of Tokens to a list of NamedLists representing the tokens.
    *
@@ -194,41 +194,97 @@ public abstract class AnalysisRequestHandlerBase extends RequestHandlerBase {
    *
    * @return List of NamedLists containing the relevant information taken from the tokens
    */
-  private List<NamedList> convertTokensToNamedLists(List<Token> tokens, AnalysisContext context) {
-    List<NamedList> tokensNamedLists = new ArrayList<NamedList>();
-
-    Collections.sort(tokens, new Comparator<Token>() {
-      public int compare(Token o1, Token o2) {
-        return o1.endOffset() - o2.endOffset();
+  private List<NamedList> convertTokensToNamedLists(final List<AttributeSource> tokens, AnalysisContext context) {
+    final List<NamedList> tokensNamedLists = new ArrayList<NamedList>();
+
+    final int[] positions = new int[tokens.size()];
+    int position = 0;    
+    for (int i = 0, c = tokens.size(); i < c; i++) {
+      AttributeSource token = tokens.get(i);
+      position += token.addAttribute(PositionIncrementAttribute.class).getPositionIncrement();
+      positions[i] = position;
+    }
+    
+    // sort the tokens by absoulte position
+    new SorterTemplate() {
+      @Override
+      protected void swap(int i, int j) {
+        Collections.swap(tokens, i, j);
+      }
+      
+      @Override
+      protected int compare(int i, int j) {
+        return positions[i] - positions[j];
       }
-    });
 
-    int position = 0;
+      @Override
+      protected void setPivot(int i) {
+        pivot = positions[i];
+      }
+  
+      @Override
+      protected int comparePivot(int j) {
+        return pivot - positions[j];
+      }
+      
+      private int pivot;
+    }.mergeSort(0, tokens.size() - 1);
 
     FieldType fieldType = context.getFieldType();
 
-    for (Token token : tokens) {
-      NamedList<Object> tokenNamedList = new SimpleOrderedMap<Object>();
+    final BytesRef rawBytes = new BytesRef();
+    final CharArr textBuf = new CharArr();
+    for (int i = 0, c = tokens.size(); i < c; i++) {
+      AttributeSource token = tokens.get(i);
+      final NamedList<Object> tokenNamedList = new SimpleOrderedMap<Object>();
+      token.getAttribute(TermToBytesRefAttribute.class).toBytesRef(rawBytes);
+
+      textBuf.reset();
+      fieldType.indexedToReadable(rawBytes, textBuf);
+      final String text = textBuf.toString();
 
-      String text = fieldType.indexedToReadable(token.toString());
       tokenNamedList.add("text", text);
-      if (!text.equals(token.toString())) {
-        tokenNamedList.add("raw_text", token.toString());
+      
+      if (token.hasAttribute(CharTermAttribute.class)) {
+        final String rawText = token.getAttribute(CharTermAttribute.class).toString();
+        if (!rawText.equals(text)) {
+          tokenNamedList.add("raw_text", rawText);
+        }
       }
-      tokenNamedList.add("type", token.type());
-      tokenNamedList.add("start", token.startOffset());
-      tokenNamedList.add("end", token.endOffset());
 
-      position += token.getPositionIncrement();
-      tokenNamedList.add("position", position);
+      tokenNamedList.add("raw_bytes", rawBytes.toString());
 
-      if (context.getTermsToMatch().contains(token.toString())) {
+      if (context.getTermsToMatch().contains(rawBytes)) {
         tokenNamedList.add("match", true);
       }
 
-      if (token.getPayload() != null) {
-        tokenNamedList.add("payload", token.getPayload());
-      }
+      tokenNamedList.add("position", positions[i]);
+
+      token.reflectWith(new AttributeReflector() {
+        public void reflect(Class<? extends Attribute> attClass, String key, Object value) {
+          // leave out position and bytes term
+          if (TermToBytesRefAttribute.class.isAssignableFrom(attClass))
+            return;
+          if (CharTermAttribute.class.isAssignableFrom(attClass))
+            return;
+          if (PositionIncrementAttribute.class.isAssignableFrom(attClass))
+            return;
+          
+          String k = attClass.getName() + '#' + key;
+          
+          // map keys for "standard attributes":
+          if (ATTRIBUTE_MAPPING.containsKey(k)) {
+            k = ATTRIBUTE_MAPPING.get(k);
+          }
+          
+          if (value instanceof Payload) {
+            final Payload p = (Payload) value;
+            value = new BytesRef(p.getData()).toString();
+          }
+
+          tokenNamedList.add(k, value);
+        }
+      });
 
       tokensNamedLists.add(tokenNamedList);
     }
@@ -261,38 +317,27 @@ public abstract class AnalysisRequestHandlerBase extends RequestHandlerBase {
    */
   // TODO refactor to support custom attributes
   protected final static class ListBasedTokenStream extends TokenStream {
-    private final List<Token> tokens;
-    private Iterator<Token> tokenIterator;
-
-    private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
-    private final OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
-    private final TypeAttribute typeAtt = addAttribute(TypeAttribute.class);
-    private final FlagsAttribute flagsAtt = addAttribute(FlagsAttribute.class);
-    private final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);
-    private final PositionIncrementAttribute posIncAtt = addAttribute(PositionIncrementAttribute.class);
+    private final List<AttributeSource> tokens;
+    private Iterator<AttributeSource> tokenIterator;
+
     /**
      * Creates a new ListBasedTokenStream which uses the given tokens as its token source.
      *
      * @param tokens Source of tokens to be used
      */
-    ListBasedTokenStream(List<Token> tokens) {
+    ListBasedTokenStream(List<AttributeSource> tokens) {
       this.tokens = tokens;
       tokenIterator = tokens.iterator();
     }
 
-    /**
-     * {@inheritDoc}
-     */
     @Override
     public boolean incrementToken() throws IOException {
       if (tokenIterator.hasNext()) {
-        Token next = tokenIterator.next();
-        termAtt.copyBuffer(next.buffer(), 0, next.length());
-        typeAtt.setType(next.type());
-        offsetAtt.setOffset(next.startOffset(), next.endOffset());
-        flagsAtt.setFlags(next.getFlags());
-        payloadAtt.setPayload(next.getPayload());
-        posIncAtt.setPositionIncrement(next.getPositionIncrement());
+        AttributeSource next = tokenIterator.next();
+        Iterator<Class<? extends Attribute>> atts = next.getAttributeClassesIterator();
+        while (atts.hasNext()) // make sure all att impls in the token exist here
+          addAttribute(atts.next());
+        next.copyTo(this);
         return true;
       } else {
         return false;
@@ -314,7 +359,7 @@ public abstract class AnalysisRequestHandlerBase extends RequestHandlerBase {
     private final String fieldName;
     private final FieldType fieldType;
     private final Analyzer analyzer;
-    private final Set<String> termsToMatch;
+    private final Set<BytesRef> termsToMatch;
 
     /**
      * Constructs a new AnalysisContext with a given field tpe, analyzer and 
@@ -328,7 +373,7 @@ public abstract class AnalysisRequestHandlerBase extends RequestHandlerBase {
      * @param termsToMatch Holds all the terms that should match during the 
      *                     analysis process.
      */
-    public AnalysisContext(FieldType fieldType, Analyzer analyzer, Set<String> termsToMatch) {
+    public AnalysisContext(FieldType fieldType, Analyzer analyzer, Set<BytesRef> termsToMatch) {
       this(null, fieldType, analyzer, termsToMatch);
     }
 
@@ -343,7 +388,7 @@ public abstract class AnalysisRequestHandlerBase extends RequestHandlerBase {
      *
      */
     public AnalysisContext(String fieldName, FieldType fieldType, Analyzer analyzer) {
-      this(fieldName, fieldType, analyzer, EMPTY_STRING_SET);
+      this(fieldName, fieldType, analyzer, EMPTY_BYTES_SET);
     }
 
     /**
@@ -359,7 +404,7 @@ public abstract class AnalysisRequestHandlerBase extends RequestHandlerBase {
      * @param termsToMatch Holds all the terms that should match during the 
      *                     analysis process.
      */
-    public AnalysisContext(String fieldName, FieldType fieldType, Analyzer analyzer, Set<String> termsToMatch) {
+    public AnalysisContext(String fieldName, FieldType fieldType, Analyzer analyzer, Set<BytesRef> termsToMatch) {
       this.fieldName = fieldName;
       this.fieldType = fieldType;
       this.analyzer = analyzer;
@@ -378,7 +423,7 @@ public abstract class AnalysisRequestHandlerBase extends RequestHandlerBase {
       return analyzer;
     }
 
-    public Set<String> getTermsToMatch() {
+    public Set<BytesRef> getTermsToMatch() {
       return termsToMatch;
     }
   }
